{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089fbb81-252b-4a37-ae57-2ac18449f52b",
   "metadata": {},
   "source": [
    "# Cognitive Mapping\n",
    "\n",
    "The data consists of text and relations, which have three parts (Concept1, Explanation and Concept2). Each of the three parts correspond to a multi-word phrase in a text. All relations and text can be easily read into a Jupyter notebook.\n",
    "\n",
    "The goal is to identify the three parts of the relation in a text automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d3328-c145-4c18-8dc5-d50c8d23ff60",
   "metadata": {},
   "source": [
    "**Example**: '3-2: <span style=\"background-color: lightblue;\">[concept Giving to the ECB the ultimate responsibility for supervision of banks in the euro area concept]</span> <span style=\"background-color: pink;\">[explanation will decisively contribute to increase explanation]</span> <span style=\"background-color: lightblue;\">[concept confidence between the banks concept]</span> <span style=\"background-color: pink;\">[explanation and in this way increase explanation]</span> <span style=\"background-color: lightblue;\">[concept the financial stability in the euro area concept]</span>. The euro area governments and the European institutions, including naturally the European Commission and the ECB, will do whatever is necessary to secure the financial stability of the euro area.\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff430e-e844-4f0f-b3b0-2ff56cc9d985",
   "metadata": {},
   "source": [
    "## 1. Plan\n",
    "    \n",
    "1. machine learning of paragraphs: do they contain a causal relation or not\n",
    "2. find phrases of relations in text: either concepts, explanations or not present in a relation\n",
    "3. identify relations based on recognized concept phrases and explanation phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513d999-5b0e-43f2-9d37-3309c833360a",
   "metadata": {},
   "source": [
    "We need a tagger or a entity recognition program, for example transformers, huggingface/bert: https://github.com/huggingface/transformers , or Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689333d-47ed-4c9c-b13a-0c54e5104b3b",
   "metadata": {},
   "source": [
    "## 1.1. Data encoding for plan step 2\n",
    "\n",
    "contribute E\\\n",
    "to E\\\n",
    "increase E\\\n",
    "confidence C\\\n",
    "between C\\\n",
    "the C\\\n",
    "banks C\\\n",
    "the X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a47808-d4d9-416f-ab61-e59f6ce2f071",
   "metadata": {},
   "source": [
    "## 1.2. References\n",
    "\n",
    "Hosseini, M.J., Chambers, N., Reddy, S., Holt, X R., Cohen, S B., Johnson, M., & Steedman, M. (2018). Learning Typed Entailment Graphs with Global Soft Constraints, Transactions of the Association for Computational Linguistics. Sizov, G. & Ozturk, P. (2013). \n",
    "\n",
    "Zornitsa Kozareva, Irina Matveeva, Gabor Melli, Vivi Nastase Zornitsa Kozareva, Irina Matveeva, Gabor Melli, Vivi Nastase (2013). Automatic Extraction of Reasoning Chains from Textual Reports. Proceedings of TextGraphs-8 Workshop “Graph-based Methods for Natural Language Processing”, Empirical Methods in Natural Language Processing. \n",
    "\n",
    "Noah Jadallah (2021). [Cause-Effect Detection for Software Requirements Based on Token Classification with BERTCause-Effect Detection for Software Requirements Based on Token Classification with BERT](\n",
    "https://colab.research.google.com/drive/14V9Ooy3aNPsRfTK88krwsereia8cfSPc?usp=sharing#scrollTo=H_kiqxjbW3lh). Seminar Natural Language Processing for Software Engineering, Winter-term 2020/2021, Technical University Munich.\n",
    "\n",
    "Erik Tjong Kim Sang and Katja Hofmann (2009). [Lexical Patterns or Dependency Patterns: Which Is Better for Hypernym Extraction?](https://ifarm.nl/erikt/papers/conll2009.pdfhttps://ifarm.nl/erikt/papers/conll2009.pdf) In: Proceedings of CoNLL-2009, Boulder, CO, USA, 2009, pages 174-182."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745012f-ea20-4006-ae0e-aff15c468406",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "\n",
    "This notebook expects three files in a subdirectory `csv`: `Map_Contents-20200726.csv`, `Speech_Contents-20210520.txt` and `Speeches-20210520.txt`. It will look for files with the speeches in the subdirectory `txt`. The names of the speech files are expected to start with the date followed by a space and the suname of the speaker (currently restricted to one word, see function `get_speech_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe46c347-e0da-4ec0-9324-1c6958d674c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674ffb63-ac11-459c-aaa6-f7c71707859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\"csv\"), 'The directory \"csv\" does not exist!'\n",
    "assert os.path.isdir(\"txt\"), 'The directory \"txt\" does not exist!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4c4617-19e7-4761-8611-7da844ce4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_contents = pd.read_csv(\"csv/Map_Contents-20200726.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d9c7dc-9eb6-4ca5-b68e-623df60ec249",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_contents = pd.read_csv(\"csv/Speech_Contents-20210520.txt\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ca5158-7645-42ff-81b3-403e9f3b9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = pd.read_csv(\"csv/Speeches-20210520.txt\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87bac0-bf3c-41b5-b515-1d89650bbc30",
   "metadata": {},
   "source": [
    "## 3. Task 1: Predict presence of causal relations in paragraphs\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. store the paragraphs in the data structure X (data) after separating punctuation from words and replacing upper case by lower case\n",
    "2. create a data structure y (labels) with True for paragraphs with causal relations and False for others\n",
    "3. predict a label for each paragraph with a machine learning model generated from the other paragraphs\n",
    "4. evaluate the results\n",
    "\n",
    "The code in this task uses the packages `fasttext` (for machine learning) and `nltk` (for language processing) \n",
    "\n",
    "The task uses limited natural language processing to prepare the data for machine leaning:\n",
    "\n",
    "1. tokenization: separate punctuation from words\n",
    "2. conversion of upper case characters to lower case\n",
    "\n",
    "Other interesting natural language preprocessing steps:\n",
    "\n",
    "3. part-of-tagging\n",
    "4. full parsing (Stanford parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d7262f-1043-4264-9bd0-995a2167e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65465642-b2f7-4579-b5a7-1fd477de01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_id(file_name, speeches):\n",
    "    try:\n",
    "        file_name_parts = file_name.split()\n",
    "        date = file_name_parts[0]\n",
    "        speaker = list(file_name_parts[1].split(\"_\")[0])\n",
    "        speaker[0] = speaker[0].upper()\n",
    "        speaker = \"\".join(speaker)\n",
    "        speech_identifier = f\"{speaker} {date}\"\n",
    "        speech_identifier = re.sub(\"Simor 2010-05-25\", \"Simor 2010-05-26\", speech_identifier)\n",
    "        return int(speeches[speeches[\"Speech_Identifier\"] == speech_identifier][\"Speech_ID\"])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332e34a5-c18f-4b02-8c46-d2b55de1d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_ids(speech_id, speech_contents):\n",
    "    paragraph_ids = {}\n",
    "    try:\n",
    "        for i, row in speech_contents[speech_contents[\"Speech_ID\"] == speech_id].iterrows():\n",
    "            paragraph_ids[row[\"Speech_Content_ID\"]] = row[\"Speech_Content_Title\"]\n",
    "    except:\n",
    "        pass\n",
    "    return paragraph_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b58ba7f-f6b2-4a2e-9911-eaa45aa0c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_paragraphs(speech_id, paragraph_ids, map_contents):\n",
    "    paragraph_values = {}\n",
    "    for i, row in map_contents[map_contents[\"Content_Speech_ID\"] == speech_id].iterrows():\n",
    "        if row[\"Content_Source_ID\"] not in paragraph_ids:\n",
    "            print(f'warning: unknown paragraph id {row[\"Content_Source_ID\"]} for document {speech_id}')\n",
    "        else:\n",
    "            paragraph_values[f'{speech_id} {paragraph_ids[row[\"Content_Source_ID\"]]}'] = True\n",
    "    return paragraph_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c3d3b8-6788-4fde-8d9e-6699c8356093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paragraphs(file_name):\n",
    "    paragraph_list = []\n",
    "    data_file = open(file_name, \"r\", encoding=\"latin1\")\n",
    "    for line in data_file:\n",
    "        paragraph_list.append(line.strip())\n",
    "    data_file.close()\n",
    "    return paragraph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e1c2a2-1f0a-453f-b77c-75a3554eff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_paragraphs(paragraph_list, paragraph_values, speech_id):\n",
    "    paragraph_texts = {}\n",
    "    for paragraph in paragraph_list:\n",
    "        tokens = paragraph.split()\n",
    "        if len(tokens) > 0 and re.search(r'^\\d+-\\d+:$', tokens[0]):\n",
    "            key = re.sub(\":\", \"\", tokens[0])\n",
    "            key = f\"{speech_id} {key}\" \n",
    "            paragraph_texts[key] = \" \".join(word_tokenize(\" \".join(tokens[1:]))).lower()\n",
    "            if key not in paragraph_values:\n",
    "                paragraph_values[key] = False\n",
    "    return paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d635990-baa2-466d-99ba-998e0134b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(speeches, speech_contents, map_contents):\n",
    "    paragraph_texts_all = {}\n",
    "    paragraph_values_all = {}\n",
    "    files = os.listdir(\"txt\")\n",
    "    for file_name in files:\n",
    "        speech_id = get_speech_id(file_name, speeches)\n",
    "        if speech_id == None:\n",
    "            print(f\"skipping file {file_name}\")\n",
    "        else:\n",
    "            paragraph_ids = get_paragraph_ids(speech_id, speech_contents)\n",
    "            paragraph_values = check_paragraphs(speech_id, paragraph_ids, map_contents)\n",
    "            paragraph_list = read_paragraphs(f\"txt/{file_name}\")\n",
    "            paragraph_texts = select_paragraphs(paragraph_list, paragraph_values, speech_id)\n",
    "            paragraph_texts_all.update(paragraph_texts)\n",
    "            paragraph_values_all.update(paragraph_values)\n",
    "    return paragraph_texts_all, paragraph_values_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af520e25-eac0-4583-baff-d45520d6ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(X, y, test_index=0):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    index = 0\n",
    "    for key in sorted(X.keys()):\n",
    "        if index == test_index:\n",
    "            test_list.append(f\"__label__{str(y[key])} {X[key]}\")\n",
    "        else:\n",
    "            train_list.append(f\"__label__{str(y[key])} {X[key]}\")\n",
    "        index += 1\n",
    "    return train_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "252e8127-40a4-493a-bc12-c9b976a5eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_file(file_name, train_list):\n",
    "    data_file = open(file_name, \"w\")\n",
    "    for line in train_list:\n",
    "        print(line, file=data_file)\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4507f68a-d422-4191-91f1-6665f29e9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_label(label):\n",
    "    return re.sub(\"__label__\", \"\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1859e3e5-7d48-4971-a28b-5c5153053bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(results):\n",
    "    return pd.DataFrame(list(results.values()), index=list(results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f4ae21e-d32c-4d7b-88aa-85b201702b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results):\n",
    "    correct_count = 0\n",
    "    for key in results:\n",
    "        if decode_label(results[key][\"predicted\"]) == str(results[key][\"correct\"]):\n",
    "            correct_count += 1\n",
    "    print(f\"correct: {round(100*correct_count/len(results), 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0617b78-e559-4d77-ad29-89054181bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_y_values(y):\n",
    "    values = {}\n",
    "    for key in y:\n",
    "        if y[key] not in values:\n",
    "            values[y[key]] = 0\n",
    "        values[y[key]] += 1\n",
    "    for key in values:\n",
    "        print(values[key], f\"{round(100*values[key]/len(y), 1)}%\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0857c3c-7cbc-4cca-abcd-5684f4713b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text):\n",
    "    clear_output(wait=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03813d8e-ee70-4d55-b70b-b18348730cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping file placeholder.txt\n"
     ]
    }
   ],
   "source": [
    "X, y = read_data(speeches, speech_contents, map_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "750bf174-9c0b-4b1b-a5ac-421ad1c285bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 80.3% True\n",
      "15 19.7% False\n"
     ]
    }
   ],
   "source": [
    "count_y_values(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c74262-f8ca-42fd-8a03-0e172ba81841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment 76 of 76\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for i in range(0, len(X)):\n",
    "    key = list(sorted(X.keys()))[i]\n",
    "    train_list, test_list = make_train_test(X, y, i)\n",
    "    make_train_file(\"train_file.txt\", train_list)\n",
    "    model = fasttext.train_supervised(\"train_file.txt\")\n",
    "    predicted_label = model.predict(test_list)\n",
    "    results[key] = {\"correct\": y[key], \"predicted\": decode_label(predicted_label[0][0][0])}\n",
    "    squeal(f\"Running experiment {i+1} of {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ba93d2c-5415-419c-bfa0-1197df0abffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 80.3%\n"
     ]
    }
   ],
   "source": [
    "evaluate_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57289f5c-6cb8-4378-a9a2-ff33b01244e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>232 2-2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232 2-3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232 3-1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232 4-1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232 4-2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536 1-2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536 2-3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536 3-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536 3-2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536 3-3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         correct predicted\n",
       "232 2-2     True      True\n",
       "232 2-3     True      True\n",
       "232 3-1     True      True\n",
       "232 4-1     True      True\n",
       "232 4-2     True      True\n",
       "...          ...       ...\n",
       "536 1-2     True      True\n",
       "536 2-3     True      True\n",
       "536 3-1    False      True\n",
       "536 3-2     True      True\n",
       "536 3-3    False      True\n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb5343-b3c8-4dc0-822c-5be29d3c178f",
   "metadata": {},
   "source": [
    "## 4. Task 2: Find relevant phrases in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db1c55-ba50-4044-a519-89575f8d6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea18e7-96ab-4d99-80a2-89699597d76c",
   "metadata": {},
   "source": [
    "### 4.1 Testing the pretrained Named Entity Recognition (NER) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94ba35-20fc-411e-81d2-0fc650d29973",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09f3b3-5cf9-4556-99f0-d6667a907958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraph_list[0])\n",
    "classifier(paragraph_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d97c29-0e51-4f80-bcdb-4065ffcab3c2",
   "metadata": {},
   "source": [
    "### 4.2 Training a phrase recognition model\n",
    "\n",
    "Source: https://huggingface.co/transformers/task_summary.html#named-entity-recognition\n",
    "\n",
    "This does not change the behaviour of the system. Perhaps we need to start from https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77dfbd-adc2-42cb-9900-19d81b30b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "label_list = [\n",
    "    \"O\",       # Outside of a phrase\n",
    "    \"I-CON\"    # Concept\n",
    "    \"I-EXP\"    # Explanation\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847361d-c5a3-4f1d-8c9c-4d7015c17a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = paragraph_list[0]\n",
    "\n",
    "# Bit of a hack to get the tokens with the special tokens\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "outputs = model(inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63354c2-f416-4bb0-8f36-9213464437ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f05f4-7291-4fc5-b1c0-4d45171c63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9c0bb-fa28-4c1b-b149-26d58f64f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d271cd5-4753-4748-be22-cf45fdcd45cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
