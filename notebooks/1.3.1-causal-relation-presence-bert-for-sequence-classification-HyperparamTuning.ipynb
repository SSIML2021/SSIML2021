{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning for Causal relation classification bert-for-sequence-classification<br>\n",
    "In this task, we will implement Hyper-parameter Tuning for transformer encoder models to classify whether a paragraph contains a causal relation, we will conduct various experiments using grid search to find what hyperparameter settings will give better performance. \n",
    "\n",
    "### Notes: \n",
    "1. The training process could be very long, it dcepends on the power of your computational resources.\n",
    "2. This notebook serves to experiment 1.3: to find the \"best\" hyperparameter settings.\n",
    "\n",
    "\n",
    "### Methodology\n",
    "\n",
    "#### Transformer encoder\n",
    "In this experiment, we will Fine-tuning a bert/roberta classifier to causal relation presence. The model architecture of Fine-Tuning Bert is as follows, and this figure is referenced from: https://mccormickml.com/2019/07/22/BERT-fine-tuning/.\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" style=\"width:600px;height:500px\"><br>\n",
    "Our first step is to tokenize the paragraphs -- break them up into word and subwords in the format BERT is comfortable with. This process contains adding [CLS] and [SEP] tokens as well as substituting tokens with their IDs (tokens2IDs).\n",
    "\n",
    "After tokenization, tokenized is a list of paragraphs -- each paragraph is represented as a list of tokens. And we want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths).\n",
    "\n",
    "However, if we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. Finally, we will re-train the bert model then validate on eval_set, the corresponding visualization will be implemented as well.\n",
    "\n",
    "#### Balance the data\n",
    "Since our data is highly imbalanced, that's means we have twice as many paragraphs with causal relations compared to paragraphs without causal relations. This issue would make our classifier guess a paragraph contains causal relation with a high probability because it would be less likely to make a mistake, but this is not what we want. Therefore we need to balance our data.<br>\n",
    "\n",
    "<h1><center>Undersampling VS  Oversampling</center></h1>\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)\n",
    "\n",
    "There are two common methods of balancing data: Undersampling and Oversampling, the former refers to the random sampling from the class which contains more data in order to make the dataset balanced. The latter is to copy data points from the class with less data, then make the dataset balanced. Both methods have their advantages and disadvantages, where Undersampling will make us discard some existing data, which will cost our model some training opportunities given our small data size. Nonetheless, oversampling \"manually\" improves the accuracy of the model because some data appear twice in the dataset, which means that the data points that appear in the test set are likely to appear in the training set as well.\n",
    "\n",
    "\n",
    "### Fine-tuning the Transformer encoder \n",
    "\n",
    "Fine-tuning the transformer is not a simple task, and we may encounter severe overfitting resulting in inadequate generalization of the model, especially when we lack a large amount of data data. Inspired from the blog: https://raphaelb.org/posts/freezing-bert/, freezing layer(s) for transformer model seems resonable when you fine tuning the transformer encoder. In addition, the method of setting a reasonable learning rate and the corresponding decay of the learning rate according to the task will also affect the training of the model. Furthermore we introduced AdamW's optimizer to prevent the occurrence of overfitting. Thus, we will conduct a series of experiments to verify which hyperparameter settings are \"optimal\" by means of grid search, and we will visualize the training, validation sets' loss values and the corresponding Macro F1 scores. The following are some of the parameters that will be involved:\n",
    "\n",
    "1. model_name: ['bert','roberta'] \n",
    "2. learning rate: [5e-5, 1e-5, 5e-6,1e-6]\n",
    "3. freeze_layer_count: [0,-1,2,4,6,8]\n",
    "4. learning rate decay methods: ['none','step']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\PythonJupyterStudy\\CM\\CM_Macro\\SSIML2021\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
    "from src.data.make_dataset import read_data_file,make_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle,resample\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import BertTokenizer,RobertaTokenizer,BertForSequenceClassification,get_linear_schedule_with_warmup,AdamW, BertConfig, RobertaForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import wandb\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\"data/annotations\"), 'The directory \"data/annotations\" does not exist!'\n",
    "assert os.path.isdir(\"data/documents\"), 'The directory \"data/documents\" does not exist!'\n",
    "map_contents = read_data_file(\"data/annotations/Map_Contents-20200726.csv\")\n",
    "speech_contents = read_data_file(\"data/annotations/Speech_Contents-20210520.txt\")\n",
    "speeches = read_data_file(\"data/annotations/Speeches-20210520.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mwarning: mismatch meta data (10) vs file (7) for file 1998-12-14 Schroeder ann.txt\u001b[0m\n",
      "skipping file in language fr: 2009-12-01 Sarkozy Elysee (Economy) ann fr.txt\n",
      "skipping file in language fr: 2009-12-14 Sarkozy Elysee (Economy) ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (2) vs file (1) for file 2010-03-29 Brown sfm2020 ann.txt\u001b[0m\n",
      "skipping file in language fr: 2010-04-20 Barroso European Commission ann fr.txt\n",
      "skipping file in language fr: 2011-01-13 Sarkozy gb ann.txt\n",
      "skipping file in language nl: 2011-04-06 Rutte FD evenement ann NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (12) for file 2011-05-20 Weidmann Deutsche Bundesbank_01 ANN.txt\u001b[0m\n",
      "skipping file in language nl: 2011-09-27 Rutte Rijksoverheid ann.txt\n",
      "skipping file in language nl: 2011-10-28 Knot dnb_01 ANN NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-10 Orban London ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-12 Rutte Trilateral Commission - code ann.txt\u001b[0m\n",
      "skipping file in language de: 2012-01-06 Rutte CSU klausurtagung ann G.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 208; file name: 2012-02-24 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (24) vs file (22) for file 2012-07-11 Rajoy La Moncloa ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (11) vs file (10) for file 2012-07-17 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "skipping file in language unk: 2012-07-26 Barroso European Commission.txt\n",
      "skipping file in language fr: 2012-08-30 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 335; file name: 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (7) vs file (6) for file 2012-12-07 Simor Bis.org ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-02-19 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (19) vs file (18) for file 2013-02-26 Rohde Danmarks Nationalbank ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-04-17 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (13) for file 2013-05-31 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (22) vs file (19) for file 2013-10-08 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2013-11-21 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language de: 2014-02-27 Merkel Bundesregerung ann g.txt\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2014-07-18 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2015-01-19 Merkel Bundesregerung ann g.txt\n",
      "skipping file placeholder.txt\n",
      "read 118 files with 897 paragraphs; skipped 16 files\n",
      "Preprocessing:\n",
      "\n",
      "19 na data found\n",
      "na data dropped\n"
     ]
    }
   ],
   "source": [
    "X, y = make_dataset(speeches, speech_contents, map_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        # Tell PyTorch to use the GPU.\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "# get divice info for torch\n",
    "device = get_device()\n",
    "\n",
    "# set seed for random, numpy, torch\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "if str(device) == 'cuda':\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "else:\n",
    "    torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_process(df,tokenizer,max_length):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    sentences = df.X.values\n",
    "    labels = df.y.values\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,   # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    #print('Check the original paragraph and converted paragrapg: ')\n",
    "    #print('Original: ', sentences[1])\n",
    "    #print('Token IDs:', input_ids[1])\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return dataset\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps):\n",
    "    print('\\nTraining and evaluating the model.')\n",
    "    \n",
    "    #store a number of quantities such as training and validation loss,validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    #store prediction and true labels\n",
    "    train_logits = []\n",
    "    train_label = []\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "    \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "    \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "    \n",
    "            model.zero_grad()        \n",
    "            loss, logits = model(b_input_ids, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=b_input_mask, \n",
    "                                 labels=b_labels)\n",
    "            \n",
    "            logits_ = logits.detach().cpu().numpy()\n",
    "            label_ids_ = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            #store prediction for the last epoch\n",
    "            if epoch_i == epochs-1:\n",
    "                train_logits.extend(logits_)\n",
    "                train_label.extend(label_ids_)\n",
    "            total_train_loss += loss.item()\n",
    "    \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "            # Update parameters and take a step using the computed gradient.Update the learning rate.\n",
    "            optimizer.step()\n",
    "        \n",
    "        #Update scheduler(lr decay) every epoch\n",
    "        lr_stat_opt = optimizer.param_groups[0][\"lr\"] #or lr_stat_scheduler = scheduler.get_last_lr()[0]\n",
    "        print('current lr is:',lr_stat_opt)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        wandb.log({\"lr\": lr_stat_opt})\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        eval_pred = []\n",
    "        eval_label = []\n",
    "        t0 = time.time()\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "            # `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                (loss, logits) = model(b_input_ids, \n",
    "                                       token_type_ids=None, \n",
    "                                       attention_mask=b_input_mask,\n",
    "                                       labels=b_labels)\n",
    "                \n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            #total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            eval_pred.extend(logits)\n",
    "            eval_label.extend(label_ids)\n",
    "        \n",
    "\n",
    "        eval_pred = convert_logits_tolabel(eval_pred)\n",
    "        f1_val = f1_score(eval_label,eval_pred,average='macro')\n",
    "        \n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        #avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Macro F1 score: {0:.2f}\".format(f1_val))\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "        \n",
    "        wandb.log({\"Training Loss\": avg_train_loss, \"Valid. Loss\":avg_val_loss,\"Valid. Macro F1\":f1_val, \"epoch\": epoch_i + 1 })\n",
    "        \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Macro F1.': f1_val,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    pd.set_option('precision', 2)\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    \n",
    "    # Plot the learning curve.\n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "    \n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    epoch_list = [i+1 for i in range(epochs)]\n",
    "    plt.xticks(epoch_list)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    train_pred = convert_logits_tolabel(train_logits)\n",
    "    \n",
    "    return model,train_pred,eval_pred,train_label,eval_label\n",
    "\n",
    "def get_prediction(df_test,model,batch_size,max_length,model_name):\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "\n",
    "    test_dataset = tokenize_process(df_test,tokenizer,max_length)\n",
    "    test_dataloader = DataLoader(test_dataset,sampler = SequentialSampler(test_dataset),batch_size = batch_size)\n",
    "    \n",
    "    test_logits = []\n",
    "    test_label = []\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        \"\"\"\n",
    "        Unpack this training batch from our dataloader.\n",
    "        `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "        \"\"\"\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].long().to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            test_logits.extend(logits)\n",
    "            test_label.extend(label_ids)\n",
    "            \n",
    "    def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred\n",
    "    \n",
    "    test_logits = convert_logits_tolabel(test_logits)\n",
    "            \n",
    "    return test_logits,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_cls(df_train,df_val,\n",
    "             epochs = 10,\n",
    "             batch_size =16,\n",
    "             max_length=128,\n",
    "             model_name='roberta',\n",
    "             lr = 5e-5,\n",
    "             weight_decay = 1e-2,\n",
    "             freeze_layer_count=1,\n",
    "             scheduler_type='step',\n",
    "             decayRate=0.75):\n",
    "    \n",
    "    \n",
    "    #wandb update\n",
    "    wandb.config.max_length = max_length\n",
    "    wandb.config.model = model_name\n",
    "    wandb.config.weight_decay = weight_decay\n",
    "    wandb.config.freeze_layer_count = freeze_layer_count\n",
    "    wandb.config.optimizer = 'AdamW'\n",
    "    wandb.config.scheduler_type = scheduler_type\n",
    "    wandb.config.decayRate = decayRate\n",
    "    \n",
    "    \n",
    "    print('\\n======================Doing Bert classification task======================')\n",
    "    \n",
    "    \"\"\"\n",
    "    step1: Tokenization\n",
    "    \"\"\"\n",
    "    #print('Do step1: Tokenization\\n')\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "    \n",
    "    train_dataset = tokenize_process(df_train,tokenizer,max_length)\n",
    "    val_dataset = tokenize_process(df_val,tokenizer,max_length)\n",
    "    \n",
    "    \"\"\"\n",
    "    step2: create dataloader for both training and eval set\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = batch_size\n",
    "    \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "    \n",
    "    # For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    step3: load bert model\n",
    "    \"\"\"\n",
    "    #print('Do step3: load bert model\\n')\n",
    "    model,optimizer,scheduler,total_steps = model_and_helper(epochs,train_dataloader,model_name,freeze_layer_count,lr,weight_decay,scheduler_type,decayRate)\n",
    "    \n",
    "    \"\"\"\n",
    "    step4: Training and evaluating\n",
    "    \"\"\"\n",
    "    #print('Do step4: Training and evaluating\\n')\n",
    "    model,train_pred,eval_pred,train_label,eval_label = Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps)\n",
    "    \n",
    "    \n",
    "    return model,train_pred,eval_pred,train_label,eval_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_and_helper(epochs,train_dataloader,\n",
    "                     model_name='roberta',\n",
    "                     freeze_layer_count=1,\n",
    "                     lr = 5e-5,\n",
    "                     weight_decay = 1e-2,\n",
    "                     scheduler_type='linear',\n",
    "                     decayRate=0.75):\n",
    "    \n",
    "    if model_name == 'bert':\n",
    "        print('\\nLoading bert model.')\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\",  num_labels = 2,  output_attentions = False, output_hidden_states = False,  return_dict = False)\n",
    "        \n",
    "        \n",
    "    elif model_name == 'roberta':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            'roberta-base', num_labels = 2, output_attentions = False, output_hidden_states = False, return_dict = False)\n",
    "        \n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Assign GPU if you have\n",
    "    if str(device)=='cuda':\n",
    "        model.cuda()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # We freeze here the embeddings of the model\n",
    "    if freeze_layer_count:\n",
    "        if model_name == 'bert':\n",
    "            for param in model.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.bert.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "        if model_name == 'roberta':\n",
    "            for param in model.roberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.roberta.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "        \n",
    "    \n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch), the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = lr, \n",
    "                      eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                      weight_decay = weight_decay \n",
    "                    )\n",
    "    \n",
    "    epochs = epochs\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # set learning rate decay\n",
    "    if scheduler_type=='linear': \n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value\n",
    "                                                num_training_steps = total_steps)\n",
    "    elif scheduler_type=='step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='exponential':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='none':\n",
    "        scheduler = None\n",
    "    \n",
    "    else:\n",
    "        raise SystemExit('Invalid scheduler_type, it could only be one of [linear, step, exponential,none] ')\n",
    "    \n",
    "\n",
    "    return model,optimizer,scheduler,total_steps\n",
    "\n",
    "def over_sampling(df,seed_val):\n",
    "    \"\"\"\n",
    "    remove Missing value first, then output two balanced dataset (Undersampling and Oversampling)\n",
    "    Input: X,y before pre-processing\n",
    "    Output: dataframes after removing missing value, Undersampling and Oversampling\n",
    "    \"\"\"\n",
    "    df_true = df[df['y'] == True]\n",
    "    df_false = df[df['y'] == False]\n",
    "\n",
    "    # Upsampling, for the class with less data, copy some data\n",
    "    df_false_upsampled = resample(df_false, random_state=seed_val, n_samples=len(df_true), replace=True)\n",
    "    df_upsampled = pd.concat([df_false_upsampled, df_true])\n",
    "    df_upsampled = shuffle(df_upsampled)\n",
    "\n",
    "    print('\\nWe totally have {} training data after oversampling.'.format(len(df_upsampled)))\n",
    "    return df_upsampled\n",
    "\n",
    "\n",
    "def transform_df(df):\n",
    "    # transform label to int\n",
    "    df.loc[df['y'] == 'True', 'y'] = 1\n",
    "    df.loc[df['y'] == 'False', 'y'] = 0\n",
    "    df.y = df.y.astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def train_val_test(train_ratio, validation_ratio, test_ratio, X, y,seed_val):\n",
    "    \"\"\"\n",
    "    remove Missing value first, then output two balanced dataset (Undersampling and Oversampling)\n",
    "    Input: X,y before pre-processing\n",
    "    Output: dataframes after removing missing value, Undersampling and Oversampling\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame({'X': pd.Series(X), 'y': pd.Series(y)})\n",
    "\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=1 - train_ratio, random_state=seed_val)\n",
    "    df_val, df_test = train_test_split(df_test, test_size=test_ratio / (test_ratio + validation_ratio),\n",
    "                                       random_state=seed_val)\n",
    "\n",
    "    X_train, y_train = df_train['X'], df_train['y']\n",
    "    X_val, y_val = df_val['X'], df_val['y']\n",
    "    X_test, y_test = df_test['X'], df_test['y']\n",
    "\n",
    "    print('[X training set shape, X validation set shape, X test set shape]:', y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "    df_train = pd.DataFrame({'X': pd.Series(X_train), 'y': pd.Series(y_train)})\n",
    "    df_train = transform_df(df_train)\n",
    "\n",
    "    # transform testset to right form\n",
    "    df_val = pd.DataFrame({'X': pd.Series(X_val), 'y': pd.Series(y_val)})\n",
    "    df_val = transform_df(df_val)\n",
    "\n",
    "    df_test = pd.DataFrame({'X': pd.Series(X_test), 'y': pd.Series(y_test)})\n",
    "    df_test = transform_df(df_test)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args):\n",
    "    \n",
    "    training_pred,training_true,evaluation_pred,evaluation_true = [],[],[],[]\n",
    "    \n",
    "    training_pred.append(train_pred)\n",
    "    training_true.append(train_label)\n",
    "    evaluation_pred.append(eval_pred)\n",
    "    evaluation_true.append(eval_label)\n",
    "    \n",
    "    def flatten(t):\n",
    "        return [item for sublist in t for item in sublist]\n",
    "    \n",
    "    evaluation_true = flatten(evaluation_true)\n",
    "    evaluation_pred = flatten(evaluation_pred)\n",
    "    training_true = flatten(training_true)\n",
    "    training_pred = flatten(training_pred)\n",
    "    \n",
    "    \n",
    "    f1_train = f1_score(training_true, training_pred,average='macro')\n",
    "    f1_val = f1_score(evaluation_true, evaluation_pred,average='macro')\n",
    "    f1_test = f1_score(test_true,test_pred,average='macro')\n",
    "    \n",
    "    wandb.log({\"f1_train\": f1_train,\"f1_val\": f1_val, \"f1_test\": f1_test})\n",
    "\n",
    "    print('\\nargs:',args)\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    #For evaluation data\n",
    "    print('classification report on test set is:\\n')\n",
    "    clas_reprt_eval = classification_report(test_true, test_pred, target_names=target_names)\n",
    "    print(clas_reprt_eval)\n",
    "    \n",
    "    print('confusion matrix on test set is:\\n')\n",
    "    cm_eval = confusion_matrix(test_true, test_pred)\n",
    "    plot_confusion_matrix(cm_eval, ['No causal relation', 'Has causal relation'], normalize=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_run(X,y,args):\n",
    "    \n",
    "    \n",
    "    df_train, df_val, df_test = train_val_test(args['train_ratio'], args['validation_ratio'], args['test_ratio'],X,y,seed_val)\n",
    "    \n",
    "    # over-sample the training set\n",
    "    df_train = over_sampling(df_train,seed_val)\n",
    "    # store the oversampled n_sample in wandb\n",
    "    wandb.config.oversampled_n_sample = len(df_train)\n",
    "    \n",
    "\n",
    "    model,train_pred,eval_pred,train_label,eval_label = transformer_cls(df_train,df_val,\n",
    "                                                                          epochs = args['epochs'],\n",
    "                                                                          batch_size =args['batch_size'],\n",
    "                                                                          max_length=args['max_length'],\n",
    "                                                                          model_name=args['model_name'],\n",
    "                                                                          lr = args['lr'],\n",
    "                                                                          weight_decay = args['weight_decay'],\n",
    "                                                                          freeze_layer_count=args['freeze_layer_count'],\n",
    "                                                                          scheduler_type=args['scheduler_type'],\n",
    "                                                                          decayRate=args['decayRate'])\n",
    "    \n",
    "    test_pred,test_true = get_prediction(df_test,model,batch_size=args['batch_size'],max_length=args['max_length'],model_name=args['model_name'])\n",
    "    \n",
    "    evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args)\n",
    "\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #clear output if you need\n",
    "    clear_output(wait=True)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Use wandb to visualize the result\n",
    "\n",
    "We determined wandb as our visualization tool. Please alter args below if you want to reproduce the experiment. \n",
    "\n",
    "Estimation of running time under current args setting:\n",
    "1. model * lr * frozen_layer * scheduler * 5mins/run\n",
    "2. The estimated time required is:   2 * 4 * 6 * 2 * 5 / 60 = 8 hours\n",
    "\n",
    "#### Fine-tuning the Transformer encoder \n",
    "\n",
    "We will conduct a series of experiments to verify which hyperparameter settings are \"optimal\" using grid search, and we will visualize the training, validation sets' loss values and the corresponding Macro F1 scores. The following are some of the parameters that will be involved:\n",
    "\n",
    "1. model_name: ['bert','roberta'] \n",
    "2. learning rate: [5e-5, 1e-5, 5e-6,1e-6]\n",
    "3. freeze_layer_count: [0,-1,2,4,6,8]\n",
    "4. learning rate decay methods: ['none','step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'batch_size': {'value': 20},\n",
      "                'decayRate': {'value': 0.75},\n",
      "                'epochs': {'value': 20},\n",
      "                'freeze_layer_count': {'values': [0, -1, 2, 4, 6, 8]},\n",
      "                'lr': {'values': [5e-05, 1e-05, 5e-06, 1e-06]},\n",
      "                'max_length': {'value': 128},\n",
      "                'model_name': {'values': ['bert', 'roberta']},\n",
      "                'scheduler_type': {'values': ['none', 'step']},\n",
      "                'test_ratio': {'value': 0.1},\n",
      "                'train_ratio': {'value': 0.75},\n",
      "                'validation_ratio': {'value': 0.15},\n",
      "                'weight_decay': {'value': 0.05}}}\n",
      "Create sweep with ID: eyflnxfh\n",
      "Sweep URL: https://wandb.ai/tan3/CM_Experiments/sweeps/eyflnxfh\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'epochs': {\n",
    "          'value': 20},\n",
    "    'batch_size': {\n",
    "          'value': 20},\n",
    "    'train_ratio': {\n",
    "          'value': 0.75},\n",
    "    'validation_ratio': {\n",
    "          'value': 0.15},\n",
    "    'test_ratio': {\n",
    "          'value': 0.10},\n",
    "    'max_length': {\n",
    "          'value': 128},\n",
    "    'model_name': {\n",
    "          'values': ['bert','roberta']},\n",
    "    'lr': {\n",
    "        'values': [5e-5, 1e-5, 5e-6,1e-6]},\n",
    "    'weight_decay': {\n",
    "          'value': 0.05},\n",
    "     'freeze_layer_count': {\n",
    "        'values': [0,-1,2,4,6,8]},\n",
    "    'scheduler_type': {\n",
    "        'values': ['none','step']},\n",
    "    'decayRate':{\n",
    "        'value': 0.75}\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = args\n",
    "pprint.pprint(sweep_config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CM_Experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_run(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(anonymous='allow',config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        transformer_run(X,y,config)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments\n",
    "#the notebook output will be clear coz RAM limitation\n",
    "wandb.agent(sweep_id, exp_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT\n",
    "\n",
    "We validated 2 different models: Roberta and Bert in our experiment with 4 different learning rates and 6 different frozen_layer setting as well as 2 learning_rate decay methods. That's a lot of experiments, we will analyze them step by step! Please check https://wandb.ai/tan3/CM_Experiments/sweeps/eyflnxfh if you want to see more results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/tan3/CM_experiments/sweeps/eyflnxfh?jupyter=true\" style=\"border:none;width:100%;height:1080px;\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to reproduce the experiment, please enter your entity, project, sweep_id (can be checked in the wandb page)\n",
    "api = wandb.Api()\n",
    "\n",
    "entity, project, sweep_id = \"tan3\", \"CM_experiments\", \"eyflnxfh\"\n",
    "sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "sweep.display(height=1080)  # you may need to zoom out to see the whole window!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/tan3/CM_experiments/reports/1-3-1-causal-relation-presence-bert-for-sequence-classification-HyperparamTuning-Report--VmlldzoxMTQ4ODA0?jupyter=true\" style=\"border:none;width:100%;height:1024px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x21fa8d70128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entity, project = \"tan3\", \"CM_experiments\"\n",
    "report_name = \"1-3-1-causal-relation-presence-bert-for-sequence-classification-HyperparamTuning-Report--VmlldzoxMTQ4ODA0\"\n",
    "url = f\"https://wandb.ai/{entity}/{project}/reports/{report_name}\"\n",
    "\n",
    "%wandb {entity}/{project}/reports/{report_name} -h 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
