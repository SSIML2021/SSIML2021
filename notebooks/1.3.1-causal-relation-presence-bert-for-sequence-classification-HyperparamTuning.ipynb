{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal relation classification: using Pre-trained BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal relation classification: Trying to find is there a causal relation in the paragraph<br>\n",
    "\n",
    "A fun example:\n",
    "<img src=\"https://i1.wp.com/boingboing.net/wp-content/uploads/2020/11/Screen-Shot-2020-11-15-at-6.15.14-AM.png?fit=1208%2C786&ssl=1\" style=\"width:400px;height:300px\">\n",
    "\n",
    "\n",
    "In this task, we will implement a bert model to classify whether a paragraph contains a causal relation. Instead of using the last hidden state([CLS] token from the last transformer layer) as a paragraph embedding to train a linear classifier on in notebook *1.2-causal-relation-presence-bert-embeddings*, now we will start with a pre-trained BERT model and retrain the full model on our data. To illustrate the difference between this approach and *1.2-causal-relation-presence-bert-embeddings*, the differences are:\n",
    "* the classifier(here we use Feed-forward Neural network with softmax, vs Logistic regression with Sigmoid) \n",
    "* we re-train the full BERT model on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\PythonJupyterStudy\\CM\\CM_Macro\\SSIML2021\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle,resample\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from src.data.make_dataset import read_data_file,make_dataset\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import tqdm\n",
    "import warnings\n",
    "from transformers import BertTokenizer,RobertaTokenizer,BertForSequenceClassification,get_linear_schedule_with_warmup,AdamW, BertConfig, RobertaForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import wandb\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "#set random seed to keep consistency between different experiments\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "if str(device) == 'cuda':\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "else:\n",
    "    torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\"csv\"), 'The directory \"csv\" does not exist!'\n",
    "assert os.path.isdir(\"txt\"), 'The directory \"txt\" does not exist!'\n",
    "map_contents = read_data_file(\"csv/Map_Contents-20200726.csv\")\n",
    "speech_contents = read_data_file(\"csv/Speech_Contents-20210520.txt\")\n",
    "speeches = read_data_file(\"csv/Speeches-20210520.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mwarning: mismatch meta data (10) vs file (7) for file 1998-12-14 Schroeder ann.txt\u001b[0m\n",
      "skipping file in language fr: 2009-12-01 Sarkozy Elysee (Economy) ann fr.txt\n",
      "skipping file in language fr: 2009-12-14 Sarkozy Elysee (Economy) ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (2) vs file (1) for file 2010-03-29 Brown sfm2020 ann.txt\u001b[0m\n",
      "skipping file in language fr: 2010-04-20 Barroso European Commission ann fr.txt\n",
      "skipping file in language fr: 2011-01-13 Sarkozy gb ann.txt\n",
      "skipping file in language nl: 2011-04-06 Rutte FD evenement ann NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (12) for file 2011-05-20 Weidmann Deutsche Bundesbank_01 ANN.txt\u001b[0m\n",
      "skipping file in language nl: 2011-09-27 Rutte Rijksoverheid ann.txt\n",
      "skipping file in language nl: 2011-10-28 Knot dnb_01 ANN NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-10 Orban London ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-12 Rutte Trilateral Commission - code ann.txt\u001b[0m\n",
      "skipping file in language de: 2012-01-06 Rutte CSU klausurtagung ann G.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 208; file name: 2012-02-24 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (24) vs file (22) for file 2012-07-11 Rajoy La Moncloa ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (11) vs file (10) for file 2012-07-17 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "skipping file in language unk: 2012-07-26 Barroso European Commission.txt\n",
      "skipping file in language fr: 2012-08-30 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 335; file name: 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (7) vs file (6) for file 2012-12-07 Simor Bis.org ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-02-19 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (19) vs file (18) for file 2013-02-26 Rohde Danmarks Nationalbank ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-04-17 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (13) for file 2013-05-31 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (22) vs file (19) for file 2013-10-08 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2013-11-21 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language de: 2014-02-27 Merkel Bundesregerung ann g.txt\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2014-07-18 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2015-01-19 Merkel Bundesregerung ann g.txt\n",
      "skipping file placeholder.txt\n",
      "read 118 files with 897 paragraphs; skipped 16 files\n"
     ]
    }
   ],
   "source": [
    "X, y = make_dataset(speeches, speech_contents, map_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Balance the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.First of all, there still are some *Missing value* paragraphs in our data, therefore we need to remove them. <br><br>\n",
    "2.In addition, our data is highly imbalanced, that's means we have twice as many paragraphs with causal relations compared to paragraphs without causal relations. This issue would make our classifier guess a paragraph contains causal relation with a high probability because it would be less likely to make a mistake, but this is not what we want. Therefore we need to balance our data.<br>\n",
    "\n",
    "<h1><center>Undersampling VS  Oversampling</center></h1>\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)\n",
    "\n",
    "There are two common methods of balancing data: Undersampling and Oversampling, the former refers to the random sampling from the class which contains more data in order to make the dataset balanced. The latter is to copy data points from the class with less data, then make the dataset balanced.<br><br>\n",
    "Both methods have their advantages and disadvantages, where Undersampling will make us discard some existing data, which will cost our model some training opportunities given our small data size. Nonetheless, oversampling \"manually\" improves the accuracy of the model because some data appear twice in the dataset, which means that the data points that appear in the test set are likely to appear in the training set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(X,y):\n",
    "    \"\"\"\n",
    "    remove Missing value first, then output two balanced dataset (Undersampling and Oversampling)\n",
    "    Input: X,y before pre-processing\n",
    "    Output: dataframes after removing missing value, Undersampling and Oversampling\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'X':pd.Series(X),'y':pd.Series(y)})\n",
    "    df_true = df[df['y'] == True]\n",
    "    df_false = df[df['y'] == False] \n",
    "    \n",
    "    #Upsampling, for the class with less data, copy some data \n",
    "    df_false_upsampled = resample(df_false,random_state=seed_val,n_samples=len(df_true),replace=True)\n",
    "    df_upsampled = pd.concat([df_false_upsampled,df_true])\n",
    "    df_upsampled = shuffle(df_upsampled)\n",
    "    \n",
    "    print('\\nWe totally have {} training data after oversampling.'.format(len(df_upsampled)))\n",
    "    return df_upsampled\n",
    "\n",
    "\n",
    "def down_sampling():\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def transform_df(df_upsampled):\n",
    "    #transform label to int\n",
    "    df_upsampled.loc[df_upsampled['y'] == 'True', 'y'] = 1\n",
    "    df_upsampled.loc[df_upsampled['y'] == 'False', 'y'] = 0\n",
    "    df_upsampled.y = df_upsampled.y.astype(int)\n",
    "    \n",
    "    #get sentences and label, will use them to do the tokenization\n",
    "    #sentences = df_upsampled.X.values\n",
    "    #labels = df_upsampled.y.values\n",
    "    return df_upsampled"
   ]
  },
  {
   "attachments": {
    "1.1.3Model_Processing_Flow-3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model processing flow\n",
    "\n",
    "![1.1.3Model_Processing_Flow-3.png](attachment:1.1.3Model_Processing_Flow-3.png)\n",
    "\n",
    "\n",
    "As shown in the Model processing flow above, we will only use over-sampling in a appropriate way: After using kfold spliting training set and test set, only the training set data will be over-sampled, which ensure that we won't have duplicate test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Re-train a end-to-end Pre-trained BERT Classification model\n",
    "\n",
    "\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" style=\"width:600px;height:500px\">\n",
    "\n",
    "Our first step is to tokenize the paragraphs -- break them up into word and subwords in the format BERT is comfortable with. This process contains adding [CLS] and [SEP] tokens as well as substituting tokens with their IDs (tokens2IDs). \n",
    "\n",
    "After tokenization, tokenized is a list of paragraphs -- each paragraph is represented as a list of tokens. And we want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths). \n",
    "\n",
    "However, if we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. Finally, we will re-train the bert model then validate on eval_set, the corresponding visualization will be implemented as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_process(df,tokenizer,max_length):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    sentences = df.X.values\n",
    "    labels = df.y.values\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,   # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    #print('Check the original paragraph and converted paragrapg: ')\n",
    "    #print('Original: ', sentences[1])\n",
    "    #print('Token IDs:', input_ids[1])\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return dataset\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps):\n",
    "    print('\\nTraining and evaluating the model.')\n",
    "    \n",
    "    #store a number of quantities such as training and validation loss,validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    #store prediction and true labels\n",
    "    train_logits = []\n",
    "    train_label = []\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "    \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "    \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "    \n",
    "            model.zero_grad()        \n",
    "            loss, logits = model(b_input_ids, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=b_input_mask, \n",
    "                                 labels=b_labels)\n",
    "            \n",
    "            logits_ = logits.detach().cpu().numpy()\n",
    "            label_ids_ = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            #store prediction for the last epoch\n",
    "            if epoch_i == epochs-1:\n",
    "                train_logits.extend(logits_)\n",
    "                train_label.extend(label_ids_)\n",
    "            total_train_loss += loss.item()\n",
    "    \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "            # Update parameters and take a step using the computed gradient.Update the learning rate.\n",
    "            optimizer.step()\n",
    "        \n",
    "        #Update scheduler(lr decay) every epoch\n",
    "        lr_stat_opt = optimizer.param_groups[0][\"lr\"] #or lr_stat_scheduler = scheduler.get_last_lr()[0]\n",
    "        print('current lr is:',lr_stat_opt)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        wandb.log({\"lr\": lr_stat_opt})\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        eval_pred = []\n",
    "        eval_label = []\n",
    "        t0 = time.time()\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "            # `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                (loss, logits) = model(b_input_ids, \n",
    "                                       token_type_ids=None, \n",
    "                                       attention_mask=b_input_mask,\n",
    "                                       labels=b_labels)\n",
    "                \n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            #total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            eval_pred.extend(logits)\n",
    "            eval_label.extend(label_ids)\n",
    "        \n",
    "\n",
    "        eval_pred = convert_logits_tolabel(eval_pred)\n",
    "        f1_val = f1_score(eval_label,eval_pred,average='macro')\n",
    "        \n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        #avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Macro F1 score: {0:.2f}\".format(f1_val))\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "        \n",
    "        wandb.log({\"Training Loss\": avg_train_loss, \"Valid. Loss\":avg_val_loss,\"Valid. Macro F1\":f1_val, \"epoch\": epoch_i + 1 })\n",
    "        \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Macro F1.': f1_val,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    pd.set_option('precision', 2)\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    \n",
    "    # Plot the learning curve.\n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "    \n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    epoch_list = [i+1 for i in range(epochs)]\n",
    "    plt.xticks(epoch_list)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    train_pred = convert_logits_tolabel(train_logits)\n",
    "    \n",
    "    return model,train_pred,eval_pred,train_label,eval_label\n",
    "\n",
    "def get_prediction(df_test,model,batch_size,max_length,model_name):\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "\n",
    "    test_dataset = tokenize_process(df_test,tokenizer,max_length)\n",
    "    test_dataloader = DataLoader(test_dataset,sampler = SequentialSampler(test_dataset),batch_size = batch_size)\n",
    "    \n",
    "    test_logits = []\n",
    "    test_label = []\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        \"\"\"\n",
    "        Unpack this training batch from our dataloader.\n",
    "        `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "        \"\"\"\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].long().to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            test_logits.extend(logits)\n",
    "            test_label.extend(label_ids)\n",
    "            \n",
    "    def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred\n",
    "    \n",
    "    test_logits = convert_logits_tolabel(test_logits)\n",
    "            \n",
    "    return test_logits,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(train_ratio = 0.75, validation_ratio = 0.15, test_ratio = 0.10):\n",
    "    \"\"\"\n",
    "    remove Missing value first, then output two balanced dataset (Undersampling and Oversampling)\n",
    "    Input: X,y before pre-processing\n",
    "    Output: dataframes after removing missing value, Undersampling and Oversampling\n",
    "    \"\"\"\n",
    "    print('Preprocessing:\\n')\n",
    "    df = pd.DataFrame({'X':pd.Series(X),'y':pd.Series(y)})\n",
    "    print('{} na data found'.format(len(df[df['X'].isna() == True].index)))\n",
    "    df = df.dropna()\n",
    "    print('na data dropped')\n",
    "    \n",
    "    df_train, df_test = train_test_split(df, test_size=1 - train_ratio, random_state = seed_val)\n",
    "    df_val, df_test = train_test_split(df_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state = seed_val) \n",
    "    \n",
    "    X_train, y_train = df_train['X'], df_train['y']\n",
    "    X_val, y_val = df_val['X'], df_val['y']\n",
    "    X_test, y_test = df_test['X'], df_test['y']\n",
    "    \n",
    "    print('[X training set shape, X validation set shape, X test set shape]:',y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "    \n",
    "    # over-sample the Training set, then transform them to right form\n",
    "    df_train_upsampled = over_sampling(X_train, y_train)\n",
    "    df_train_upsampled = transform_df(df_train_upsampled)\n",
    "    \n",
    "    # transform testset to right form\n",
    "    df_val = pd.DataFrame({'X':pd.Series(X_val),'y':pd.Series(y_val)})\n",
    "    df_val = transform_df(df_val)\n",
    "    \n",
    "    df_test = pd.DataFrame({'X':pd.Series(X_test),'y':pd.Series(y_test)})\n",
    "    df_test = transform_df(df_test)\n",
    "    \n",
    "    \n",
    "    return df_train_upsampled, df_val, df_test\n",
    "\n",
    "def transformer_cls(df_train,df_val,\n",
    "             epochs = 10,\n",
    "             batch_size =16,\n",
    "             max_length=128,\n",
    "             model_name='roberta',\n",
    "             lr = 5e-5,\n",
    "             weight_decay = 1e-2,\n",
    "             freeze_layer_count=1,\n",
    "             scheduler_type='step',\n",
    "             decayRate=0.75):\n",
    "    \n",
    "    \n",
    "    #wandb update\n",
    "    wandb.config.max_length = max_length\n",
    "    wandb.config.model = model_name\n",
    "    wandb.config.weight_decay = weight_decay\n",
    "    wandb.config.freeze_layer_count = freeze_layer_count\n",
    "    wandb.config.optimizer = 'AdamW'\n",
    "    wandb.config.scheduler_type = scheduler_type\n",
    "    wandb.config.decayRate = decayRate\n",
    "    \n",
    "    \n",
    "    print('\\n======================Doing Bert classification task======================')\n",
    "    \n",
    "    \"\"\"\n",
    "    step1: Tokenization\n",
    "    \"\"\"\n",
    "    #print('Do step1: Tokenization\\n')\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "    \n",
    "    train_dataset = tokenize_process(df_train,tokenizer,max_length)\n",
    "    val_dataset = tokenize_process(df_val,tokenizer,max_length)\n",
    "    \n",
    "    \"\"\"\n",
    "    step2: create dataloader for both training and eval set\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = batch_size\n",
    "    \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "    \n",
    "    # For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    step3: load bert model\n",
    "    \"\"\"\n",
    "    #print('Do step3: load bert model\\n')\n",
    "    model,optimizer,scheduler,total_steps = model_and_helper(epochs,train_dataloader,model_name,freeze_layer_count,lr,weight_decay,scheduler_type,decayRate)\n",
    "    \n",
    "    \"\"\"\n",
    "    step4: Training and evaluating\n",
    "    \"\"\"\n",
    "    #print('Do step4: Training and evaluating\\n')\n",
    "    model,train_pred,eval_pred,train_label,eval_label = Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps)\n",
    "    \n",
    "    \n",
    "    return model,train_pred,eval_pred,train_label,eval_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_and_helper(epochs,train_dataloader,\n",
    "                     model_name='roberta',\n",
    "                     freeze_layer_count=1,\n",
    "                     lr = 5e-5,\n",
    "                     weight_decay = 1e-2,\n",
    "                     scheduler_type='linear',\n",
    "                     decayRate=0.75):\n",
    "    \n",
    "    if model_name == 'bert':\n",
    "        print('\\nLoading bert model.')\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\",  num_labels = 2,  output_attentions = False, output_hidden_states = False,  return_dict = False)\n",
    "        \n",
    "        \n",
    "    elif model_name == 'roberta':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            'roberta-base', num_labels = 2, output_attentions = False, output_hidden_states = False, return_dict = False)\n",
    "        \n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Assign GPU if you have\n",
    "    if str(device)=='cuda':\n",
    "        model.cuda()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # We freeze here the embeddings of the model\n",
    "    if freeze_layer_count:\n",
    "        if model_name == 'bert':\n",
    "            for param in model.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.bert.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "        if model_name == 'roberta':\n",
    "            for param in model.roberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.roberta.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "        \n",
    "    \n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch), the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = lr, \n",
    "                      eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                      weight_decay = weight_decay \n",
    "                    )\n",
    "    \n",
    "    epochs = epochs\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # set learning rate decay\n",
    "    if scheduler_type=='linear': \n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value\n",
    "                                                num_training_steps = total_steps)\n",
    "    elif scheduler_type=='step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='exponential':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='none':\n",
    "        scheduler = None\n",
    "    \n",
    "    else:\n",
    "        raise SystemExit('Invalid scheduler_type, it could only be one of [linear, step, exponential,none] ')\n",
    "    \n",
    "\n",
    "    return model,optimizer,scheduler,total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args):\n",
    "    \n",
    "    training_pred,training_true,evaluation_pred,evaluation_true = [],[],[],[]\n",
    "    \n",
    "    training_pred.append(train_pred)\n",
    "    training_true.append(train_label)\n",
    "    evaluation_pred.append(eval_pred)\n",
    "    evaluation_true.append(eval_label)\n",
    "    \n",
    "    def flatten(t):\n",
    "        return [item for sublist in t for item in sublist]\n",
    "    \n",
    "    evaluation_true = flatten(evaluation_true)\n",
    "    evaluation_pred = flatten(evaluation_pred)\n",
    "    training_true = flatten(training_true)\n",
    "    training_pred = flatten(training_pred)\n",
    "    \n",
    "    \n",
    "    f1_train = f1_score(training_true, training_pred,average='macro')\n",
    "    f1_val = f1_score(evaluation_true, evaluation_pred,average='macro')\n",
    "    f1_test = f1_score(test_true,test_pred,average='macro')\n",
    "    \n",
    "    wandb.log({\"f1_train\": f1_train,\"f1_val\": f1_val, \"f1_test\": f1_test})\n",
    "\n",
    "    print('\\nargs:',args)\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    #For evaluation data\n",
    "    print('classification report on test set is:\\n')\n",
    "    clas_reprt_eval = classification_report(test_true, test_pred, target_names=target_names)\n",
    "    print(clas_reprt_eval)\n",
    "    \n",
    "    print('confusion matrix on test set is:\\n')\n",
    "    cm_eval = confusion_matrix(test_true, test_pred)\n",
    "    plot_confusion_matrix(cm_eval, ['No causal relation', 'Has causal relation'], normalize=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_run(X,y,args):\n",
    "    \n",
    "    \n",
    "    df_train_upsampled, df_val, df_test = train_val_test(args['train_ratio'], args['validation_ratio'], args['test_ratio'])\n",
    "    \n",
    "\n",
    "    model,train_pred,eval_pred,train_label,eval_label = transformer_cls(df_train_upsampled,df_val,\n",
    "                                                                          epochs = args['epochs'],\n",
    "                                                                          batch_size =args['batch_size'],\n",
    "                                                                          max_length=args['max_length'],\n",
    "                                                                          model_name=args['model_name'],\n",
    "                                                                          lr = args['lr'],\n",
    "                                                                          weight_decay = args['weight_decay'],\n",
    "                                                                          freeze_layer_count=args['freeze_layer_count'],\n",
    "                                                                          scheduler_type=args['scheduler_type'],\n",
    "                                                                          decayRate=args['decayRate'])\n",
    "    \n",
    "    test_pred,test_true = get_prediction(df_test,model,batch_size=args['batch_size'],max_length=args['max_length'],model_name=args['model_name'])\n",
    "    \n",
    "    evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args)\n",
    "\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #clear output if you need\n",
    "    clear_output(wait=True)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the Transformer encoder \n",
    "\n",
    "Fine-tuning the transformer is not a simple task, and we may encounter severe overfitting resulting in inadequate generalization of the model, especially when we lack a large amount of data data. Inspired from the blog: https://raphaelb.org/posts/freezing-bert/, freezing layer(s) for transformer model seems resonable when you fine tuning the transformer encoder. In addition, the method of setting a reasonable learning rate and the corresponding decay of the learning rate according to the task will also affect the training of the model. Furthermore we introduced AdamW's optimizer to prevent the occurrence of overfitting. Thus, we will conduct a series of experiments to verify which hyperparameter settings are \"optimal\" by means of grid search, and we will visualize the training, validation sets' loss values and the corresponding Macro F1 scores. The following are some of the parameters that will be involved:\n",
    "\n",
    "\n",
    "\n",
    "model_name: ['bert','roberta']\n",
    "learning rate: [5e-5, 1e-5, 5e-6,1e-6]\n",
    "freeze_layer_count: [0,-1,2,4,6,8]\n",
    "learning rate decay methods: ['none','step']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use wandb to visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model * lr * frozen_layer * scheduler * 5mins/run\n",
    "##### The estimated time required is:   2 * 4 * 6 * 2 * 5 / 60 = 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'batch_size': {'value': 20},\n",
      "                'decayRate': {'value': 0.75},\n",
      "                'epochs': {'value': 20},\n",
      "                'freeze_layer_count': {'values': [0, -1, 2, 4, 6, 8]},\n",
      "                'lr': {'values': [5e-05, 1e-05, 5e-06, 1e-06]},\n",
      "                'max_length': {'value': 128},\n",
      "                'model_name': {'values': ['bert', 'roberta']},\n",
      "                'scheduler_type': {'values': ['none', 'step']},\n",
      "                'test_ratio': {'value': 0.1},\n",
      "                'train_ratio': {'value': 0.75},\n",
      "                'validation_ratio': {'value': 0.15},\n",
      "                'weight_decay': {'value': 0.05}}}\n",
      "Create sweep with ID: eyflnxfh\n",
      "Sweep URL: https://wandb.ai/tan3/CM_Experiments/sweeps/eyflnxfh\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'epochs': {\n",
    "          'value': 20},\n",
    "    'batch_size': {\n",
    "          'value': 20},\n",
    "    'train_ratio': {\n",
    "          'value': 0.75},\n",
    "    'validation_ratio': {\n",
    "          'value': 0.15},\n",
    "    'test_ratio': {\n",
    "          'value': 0.10},\n",
    "    'max_length': {\n",
    "          'value': 128},\n",
    "    'model_name': {\n",
    "          'values': ['bert','roberta']},\n",
    "    'lr': {\n",
    "        'values': [5e-5, 1e-5, 5e-6,1e-6]},\n",
    "    'weight_decay': {\n",
    "          'value': 0.05},\n",
    "     'freeze_layer_count': {\n",
    "        'values': [0,-1,2,4,6,8]},\n",
    "    'scheduler_type': {\n",
    "        'values': ['none','step']},\n",
    "    'decayRate':{\n",
    "        'value': 0.75}\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = args\n",
    "pprint.pprint(sweep_config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CM_Experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_run(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(anonymous='allow',config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        transformer_run(X,y,config)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4764... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>█▆▄▅▃▅▃▅▂▆▂▃▄▃▄▁▃▂▁▄</td></tr><tr><td>Valid. Loss</td><td>█▆▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid. Macro F1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_test</td><td>▁</td></tr><tr><td>f1_train</td><td>▁</td></tr><tr><td>f1_val</td><td>▁</td></tr><tr><td>lr</td><td>█▆▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>0.69547</td></tr><tr><td>Valid. Loss</td><td>0.71067</td></tr><tr><td>Valid. Macro F1</td><td>0.22353</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>f1_test</td><td>0.17757</td></tr><tr><td>f1_train</td><td>0.43478</td></tr><tr><td>f1_val</td><td>0.22353</td></tr><tr><td>lr</td><td>0.0</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">visionary-sweep-96</strong>: <a href=\"https://wandb.ai/tan3/CM_Experiments/runs/jg1pbb9w\" target=\"_blank\">https://wandb.ai/tan3/CM_Experiments/runs/jg1pbb9w</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211008_044916-jg1pbb9w\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Sweep Agent: Waiting for job.\n",
      "wandb: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "#the notebook output will be clear coz RAM limitation\n",
    "wandb.agent(sweep_id, exp_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT\n",
    "\n",
    "We validated 2 different models: Roberta and Bert in our experiment with 4 different learning rates and 6 different frozen_layer setting as well as 2 learning_rate decay methods. That's a lot of experiments, we will analyze them step by step!\n",
    "\n",
    "Please check https://wandb.ai/tan3/CM_Experiments?workspace=user-tan3 if you want to see more results.\n",
    "\n",
    "### How many layers should we freeze?\n",
    "\n",
    "Lets see the visualization results:\n",
    "\n",
    "As can be seen, the box plots below shows the Macro F1 score of both models on validation set regarding different learning rates. That is, it reveals under different learning rates, the distribution of macro f1 scores under the influence of different number of frozen layers.\n",
    "\n",
    "for the roberta model, freezing 6 layers leads to both highest maximium and median value, while freezing 0 layer yield the most stable result(lowest variance). On the other hand, for the bert model, the highest maxium macro F1 score yielded by the run of freezing 0 layer, followed by the one of freezing 6 layers. However, we observed  For both models, freezing 8 layers seems to be the worst option because it has the largest variance.\n",
    "\n",
    "Therefore, it is reasonable for us to freeze 6 for roberta model, and 0/6 layer(s) for bert model in the later experiment(s). For the sake of simplicity, we will only choose the non-freezing version for subsequent experiments with Bert.\n",
    "\n",
    "\n",
    "|<img src=\"Pics_CM\\F1_Bert_ validation_lrs.png\" style=\"width:600px;height:300px\">|<img src=\"Pics_CM\\F1_Roberta_ validation_lrs.png\" style=\"width:600px;height:300px\"> |\n",
    "|-|-|\n",
    "\n",
    "#### Results of roberta(frozen_layers=6)\n",
    "\n",
    "Then, lets see the result of roberta(frozen_layers=6). As can be seen, graphs below shows the losses on Training and Validation sets, as well as the Macro F1 score on validation and test sets. \n",
    "\n",
    "Note: Because it is difficult to see the curve clearly due to the number of runs, we filtered out those runs with large loss magnitude and high risk of overfitting.\n",
    "\n",
    "|<img src=\"Pics_CM\\Train_Loss_roberta_6_filtered.png\" style=\"width:600px;height:250px\">|<img src=\"Pics_CM\\val_Loss_roberta_6_filtered.png\" style=\"width:600px;height:250px\"> |\n",
    "|-|-|\n",
    "\n",
    "|<img src=\"Pics_CM\\val_f1_roberta_6_filtered.png\" style=\"width:650px;height:250px\">| <img src=\"Pics_CM\\f1_test_roberta_6_filtered.png\" style=\"width:600px;height:250px\">|\n",
    "|-|-|\n",
    "\n",
    "\n",
    "Note: The colors of runs in the graph below are not corresponding to those colors in the graphs above. The graph only shows the settings of hyper-parameters in those runs.\n",
    "\n",
    "<img src=\"Pics_CM\\special_roberta_6_filtered.png\" style=\"width:950px;height:300px\">\n",
    "\n",
    "#### Results of bert(frozen_layers=0)\n",
    "\n",
    "Then, lets see the result of bert(frozen_layers=0). As can be seen, graphs below shows the losses on Training and Validation sets, as well as the Macro F1 score on validation and test sets. \n",
    "\n",
    "Note: Because it is difficult to see the curve clearly due to the number of runs, we filtered out those runs with large loss magnitude and high risk of overfitting.\n",
    "\n",
    "|<img src=\"Pics_CM\\Train_Loss_bert_0_filtered.png\" style=\"width:600px;height:250px\">|<img src=\"Pics_CM\\val_Loss_bert_0_filtered.png\" style=\"width:600px;height:250px\"> |\n",
    "|-|-|\n",
    "\n",
    "|<img src=\"Pics_CM\\val_f1_bert_0_filtered.png\" style=\"width:650px;height:250px\">| <img src=\"Pics_CM\\f1_test_bert_0_filtered.png\" style=\"width:600px;height:250px\">|\n",
    "|-|-|\n",
    "\n",
    "\n",
    "Note: The colors of runs in the graph below are not corresponding to those colors in the graphs above. The graph only shows the settings of hyper-parameters in those runs.\n",
    "\n",
    "<img src=\"Pics_CM\\special_bert_0_filtered.png\" style=\"width:950px;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Hyper-parameter settings is the optimal one?\n",
    "\n",
    "To verify what hyperparameter settings the model has better performance and generalization ability, we visualize a total of six settings together for the bert and roberta models in above figures. It seems that \"unqiue-sweep-72\" is the optimal model, where it achieves more than 70% of the Macro F1 score in the validation set and 63.43% in the test set (even if it is not optimal). However, we cannot exclude that his validation loss exhibits some overfitting, even if the degree is not significant (from 0.68 down to 0.75). We cannot exclude that the overfitting may be due to the small amount of data, so in 1.4 we will perform a sensitivity analysis based on different amounts of data.\n",
    "\n",
    "|<img src=\"Pics_CM\\Best_six_training.png\" style=\"width:550px;height:250px\">| <img src=\"Pics_CM\\Best_six_validation.png\" style=\"width:550px;height:250px\">|\n",
    "|-|-|\n",
    "\n",
    "<img src=\"Pics_CM\\Best_six_test.png\" style=\"width:550px;height:250px\">\n",
    "\n",
    "<img src=\"Pics_CM\\best_loss_f1.png\" style=\"width:800px;height:250px\">\n",
    "\n",
    "The hyperparameters settings of the optimal model are:\n",
    "\n",
    "best_args = {\n",
    "    'epochs': {\n",
    "          'value': 14},\n",
    "    'batch_size': {\n",
    "          'value': 20},\n",
    "    'train_ratio': {\n",
    "          'value': 0.75},\n",
    "    'validation_ratio': {\n",
    "          'value': 0.15},\n",
    "    'test_ratio': {\n",
    "          'value': 0.10},\n",
    "    'max_length': {\n",
    "          'value': 128},\n",
    "    'model_name': {\n",
    "          'value': 'roberta'},\n",
    "    'lr': {\n",
    "        'value': 1e-05},\n",
    "    'weight_decay': {\n",
    "          'value': 0.05},\n",
    "     'freeze_layer_count': {\n",
    "        'value': 6},\n",
    "    'scheduler_type': {\n",
    "        'value': 'step'},\n",
    "    'decayRate':{\n",
    "        'value': 0.75}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
