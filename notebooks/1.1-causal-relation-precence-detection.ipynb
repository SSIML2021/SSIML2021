{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089fbb81-252b-4a37-ae57-2ac18449f52b",
   "metadata": {},
   "source": [
    "# Paragraph causal relation detection: do they contain a causal relation or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d3328-c145-4c18-8dc5-d50c8d23ff60",
   "metadata": {},
   "source": [
    "**Example**: '3-2: <span style=\"background-color: lightblue;\">[concept Giving to the ECB the ultimate responsibility for supervision of banks in the euro area concept]</span> <span style=\"background-color: pink;\">[explanation will decisively contribute to increase explanation]</span> <span style=\"background-color: lightblue;\">[concept confidence between the banks concept]</span> <span style=\"background-color: pink;\">[explanation and in this way increase explanation]</span> <span style=\"background-color: lightblue;\">[concept the financial stability in the euro area concept]</span>. The euro area governments and the European institutions, including naturally the European Commission and the ECB, will do whatever is necessary to secure the financial stability of the euro area.\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745012f-ea20-4006-ae0e-aff15c468406",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "This notebook expects three files in a subdirectory `csv`: `Map_Contents-20200726.csv`, `Speech_Contents-20210520.txt` and `Speeches-20210520.txt`. It will look for files with the speeches in the subdirectory `txt`. The names of the speech files are expected to start with the date followed by a space and the suname of the speaker (currently restricted to one word, see function `get_speech_id`).\n",
    "\n",
    "If you are just interested in learning what the code is doing, you can skip all code blocks with the commands `import` (load libraries), `assert` (perform tests) and `def` (define functions), and examine the other code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe46c347-e0da-4ec0-9324-1c6958d674c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.data.make_dataset import read_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674ffb63-ac11-459c-aaa6-f7c71707859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\"csv\"), 'The directory \"csv\" does not exist!'\n",
    "assert os.path.isdir(\"txt\"), 'The directory \"txt\" does not exist!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4c4617-19e7-4761-8611-7da844ce4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_contents = read_data_file(\"csv/Map_Contents-20200726.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d9c7dc-9eb6-4ca5-b68e-623df60ec249",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_contents = read_data_file(\"csv/Speech_Contents-20210520.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ca5158-7645-42ff-81b3-403e9f3b9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = read_data_file(\"csv/Speeches-20210520.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87bac0-bf3c-41b5-b515-1d89650bbc30",
   "metadata": {},
   "source": [
    "## 2. Predict presence of causal relations in paragraphs\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. store the paragraphs in the data structure X (data) after separating punctuation from words and replacing upper case by lower case\n",
    "2. create a data structure y (labels) with True for paragraphs with causal relations and False for others\n",
    "3. predict a label for each paragraph with a machine learning model generated from the other paragraphs\n",
    "4. evaluate the results\n",
    "\n",
    "The code in this task uses the packages `fasttext` (for machine learning) and `nltk` (for language processing) \n",
    "\n",
    "The task uses limited natural language processing to prepare the data for machine leaning:\n",
    "\n",
    "1. tokenization: separate punctuation from words\n",
    "2. conversion of upper case characters to lower case\n",
    "\n",
    "Other interesting natural language preprocessing steps:\n",
    "\n",
    "3. part-of-tagging\n",
    "4. full parsing (Stanford parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9885816-1ca5-4eab-ab34-6defacea8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d7262f-1043-4264-9bd0-995a2167e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from termcolor import colored\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af520e25-eac0-4583-baff-d45520d6ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(X, y, test_index=0):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    index = 0\n",
    "    for key in sorted(X.keys()):\n",
    "        if index == test_index:\n",
    "            test_list.append(f\"__label__{str(y[key])} {X[key]}\")\n",
    "        else:\n",
    "            train_list.append(f\"__label__{str(y[key])} {X[key]}\")\n",
    "        index += 1\n",
    "    return train_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "252e8127-40a4-493a-bc12-c9b976a5eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_file(file_name, train_list):\n",
    "    data_file = open(file_name, \"w\")\n",
    "    for line in train_list:\n",
    "        print(line, file=data_file)\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4507f68a-d422-4191-91f1-6665f29e9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_label(label):\n",
    "    return re.sub(\"__label__\", \"\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1859e3e5-7d48-4971-a28b-5c5153053bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(results):\n",
    "    result_list = []\n",
    "    for key in results:\n",
    "        result_list.append({\"paragraph\": key})\n",
    "        result_list[-1].update(results[key])\n",
    "    return pd.DataFrame(result_list, index=[\"\"]*len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f4ae21e-d32c-4d7b-88aa-85b201702b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results):\n",
    "    correct = 0\n",
    "    found = 0\n",
    "    target = 0\n",
    "    accuracy_count = 0\n",
    "    for key in results:\n",
    "        if results[key][\"predicted\"] == str(True):\n",
    "            found += 1\n",
    "            if results[key][\"predicted\"] == str(results[key][\"correct\"]):\n",
    "                correct += 1\n",
    "        if str(results[key][\"correct\"]) == str(True):\n",
    "            target += 1\n",
    "        if results[key][\"predicted\"] == str(results[key][\"correct\"]):\n",
    "            accuracy_count += 1\n",
    "    precision = round(100*correct/found, 1)\n",
    "    recall = round(100*correct/target, 1)\n",
    "    f = round(2 * precision * recall / (precision + recall), 1)\n",
    "    accuracy = round(100 * accuracy_count / len(results), 1)\n",
    "    print(f\"precision: {precision}%; recall: {recall}%; F: {f}; accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0617b78-e559-4d77-ad29-89054181bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_y_values(y):\n",
    "    values = {}\n",
    "    for key in y:\n",
    "        if y[key] not in values:\n",
    "            values[y[key]] = 0\n",
    "        values[y[key]] += 1\n",
    "    table = []\n",
    "    for key in values:\n",
    "        table.append({\"label\": key, \"count\": values[key], \"percentage\": f\"{round(100*values[key]/len(y), 1)}%\"})\n",
    "    return pd.DataFrame(table, index=[\"\"]*len(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0857c3c-7cbc-4cca-abcd-5684f4713b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text):\n",
    "    clear_output(wait=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fde81ba8-d96b-47b1-aa8b-4cbbbba8ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fasttext_data(data_in):\n",
    "    data_out = []\n",
    "    for paragraph, label in data_in:\n",
    "        data_out.append(\"__label__\" + str(label) + \" \" + paragraph)\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "521e816f-1dde-47d0-a285-9e96683fa0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(X, y, wordNgrams=1, pretrainedVectors=\"\"):\n",
    "    predicted_labels_all = []\n",
    "    correct_labels = [ y[key] for key in X if key in y]\n",
    "    counter = 0\n",
    "    data = np.array([ (X[key], y[key]) for key in X if key in y])\n",
    "    for train_items, test_items in sklearn.model_selection.KFold(n_splits=10).split(X):\n",
    "        train_data = make_fasttext_data(data[train_items])\n",
    "        make_train_file(\"train_file.txt\", train_data)\n",
    "        model = fasttext.train_supervised(\"train_file.txt\", dim=300, pretrainedVectors=pretrainedVectors, wordNgrams=wordNgrams)\n",
    "        test_data = make_fasttext_data(data[test_items])\n",
    "        predicted_labels = model.predict(test_data)\n",
    "        predicted_labels_all.extend(predicted_labels[0])\n",
    "        counter += 1\n",
    "        squeal(f\"Ran experiment {counter} of 10\")\n",
    "    return { i: { \"correct\": correct_labels[i], \"predicted\": decode_label(predicted_labels_all[i][0]) } for i in range(0, len(predicted_labels_all)) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c7243-9f48-4f2d-ad4c-cd66c671f45e",
   "metadata": {},
   "source": [
    "The variable `X` in the next code block contains all the data (paragraphs) available for machine learning. The variable `y` contains all the associated labels, with values `True` or `False`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03813d8e-ee70-4d55-b70b-b18348730cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping file in language de: 2015-01-19 Merkel Bundesregerung ann g.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (12) for file 2011-05-20 Weidmann Deutsche Bundesbank_01 ANN.txt\u001b[0m\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 208; file name: 2012-02-24 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-10 Orban London ann.txt\u001b[0m\n",
      "skipping file in language nl: 2011-09-27 Rutte Rijksoverheid ann.txt\n",
      "\u001b[31mwarning: mismatch meta data (7) vs file (6) for file 2012-12-07 Simor Bis.org ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (2) vs file (1) for file 2010-03-29 Brown sfm2020 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 335; file name: 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (11) vs file (10) for file 2012-07-17 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (22) vs file (19) for file 2013-10-08 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language nl: 2011-10-28 Knot dnb_01 ANN NL.txt\n",
      "skipping file in language fr: 2013-04-17 Hollande SFM2020 ann fr.txt\n",
      "skipping file in language fr: 2010-04-20 Barroso European Commission ann fr.txt\n",
      "skipping file in language de: 2012-01-06 Rutte CSU klausurtagung ann G.txt\n",
      "\u001b[31mwarning: mismatch meta data (19) vs file (18) for file 2013-02-26 Rohde Danmarks Nationalbank ann.txt\u001b[0m\n",
      "skipping file in language nl: 2011-04-06 Rutte FD evenement ann NL.txt\n",
      "skipping file in language fr: 2011-01-13 Sarkozy gb ann.txt\n",
      "skipping file in language fr: 2012-08-30 Hollande SFM2020 ann fr.txt\n",
      "skipping file in language fr: 2009-12-01 Sarkozy Elysee (Economy) ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2014-07-18 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file placeholder.txt\n",
      "skipping file in language de: 2013-11-21 Merkel Bundesregerung ann g.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (13) for file 2013-05-31 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (24) vs file (22) for file 2012-07-11 Rajoy La Moncloa ann.txt\u001b[0m\n",
      "skipping file in language unk: 2012-07-26 Barroso European Commission.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-12 Rutte Trilateral Commission - code ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-02-19 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (10) vs file (7) for file 1998-12-14 Schroeder ann.txt\u001b[0m\n",
      "skipping file in language fr: 2009-12-14 Sarkozy Elysee (Economy) ann fr.txt\n",
      "skipping file in language de: 2014-02-27 Merkel Bundesregerung ann g.txt\n",
      "read 118 files with 897 paragraphs; skipped 16 files\n"
     ]
    }
   ],
   "source": [
    "X, y = make_dataset(speeches, speech_contents, map_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "750bf174-9c0b-4b1b-a5ac-421ad1c285bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>True</td>\n",
       "      <td>660</td>\n",
       "      <td>73.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>False</td>\n",
       "      <td>237</td>\n",
       "      <td>26.4%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  count percentage\n",
       "   True    660      73.6%\n",
       "  False    237      26.4%"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_y_values(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d61a5-b403-45ed-8a18-538f8c1c64e0",
   "metadata": {},
   "source": [
    "We run as many machine learning experiments as there are paragraphs. In each machine learning experiment, one paragraph is used as test data while all others are used for training a machine learning model to predict the label of this paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "54c2afdf-3241-40a8-b648-7d7187f9d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran experiment 10 of 10\n"
     ]
    }
   ],
   "source": [
    "results = run_experiments(X, y, wordNgrams=3, pretrainedVectors=\"wiki.en.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9ba93d2c-5415-419c-bfa0-1197df0abffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 77.0%; recall: 92.8%; F: 84.2; accuracy: 74.5%\n"
     ]
    }
   ],
   "source": [
    "evaluate_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc274d89-85b1-48e4-8e7e-7a0dc2d576af",
   "metadata": {},
   "source": [
    "| ngrams | language model | precision | recall | F    | accuracy |\n",
    "| ------ | -------------- | --------- | ------ | ---- | -------- |\n",
    "| 1      | yes            | 80.0%     | 85.5%  | 82.7 | 73.8%    |\n",
    "| 2      | yes            | 78.1%     | 90.3%  | 83.8 | 74.5%    |\n",
    "| 3      | yes            | 77.0%     | 92.8%  | 84.2 | 74.5%    |\n",
    "| 1      | no             | 73.3%     | 100.%  | 84.6 | 73.3%    |\n",
    "| 2      | no             | 73.0%     | 100.%  | 84.4 | 73.0%    |\n",
    "| 3      | no             | 73.0%     | 100.%  | 84.4 | 73.0%    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d271cd5-4753-4748-be22-cf45fdcd45cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
