{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    json_data = []\n",
    "    infile = open(file_name, \"r\")\n",
    "    for line in infile:\n",
    "        json_data.append(json.loads(line))\n",
    "    infile.close()\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(json_data):\n",
    "    combined_data = {}\n",
    "    for data in json_data:\n",
    "        key = f\"{data['source_id']} {data['speech_id']} {data['paragraph_id']}\"\n",
    "        if key not in combined_data:\n",
    "            combined_data[key] = copy.deepcopy(data)\n",
    "        else:\n",
    "            if len(data[\"data\"]) != len(combined_data[key][\"data\"]):\n",
    "                print(\"cannot happen\")\n",
    "            for label_data in data[\"label\"]:\n",
    "                if label_data not in combined_data[key][\"label\"]:\n",
    "                    combined_data[key][\"label\"].append(label_data)\n",
    "                    if combined_data[key][\"label\"][-1][1] > len(combined_data[key][\"data\"]):\n",
    "                        combined_data[key][\"label\"][-1][1] = len(combined_data[key][\"data\"])\n",
    "    for key in combined_data:\n",
    "        for label_data in combined_data[key][\"label\"]:\n",
    "            label_data.append(combined_data[key][\"data\"][label_data[0]:label_data[1]])\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = read_data(\"../data/femke.jsonl\")\n",
    "combined_data = combine_data(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve features with stanza\n",
    "Stanza gives us al kind of tags. We also use it for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e3a757b36c412e8058b5b5bf61a749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 10:58:23 INFO: Downloading default packages for language: en (English)...\n",
      "2021-10-15 10:58:24 INFO: File exists: /home/dafne/stanza_resources/en/default.zip.\n",
      "2021-10-15 10:58:29 INFO: Finished downloading models and saved to /home/dafne/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 10:59:42 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-10-15 10:59:42 INFO: Use device: cpu\n",
      "2021-10-15 10:59:42 INFO: Loading: tokenize\n",
      "2021-10-15 10:59:42 INFO: Loading: pos\n",
      "2021-10-15 10:59:43 INFO: Loading: lemma\n",
      "2021-10-15 10:59:44 INFO: Loading: depparse\n",
      "2021-10-15 10:59:45 INFO: Loading: ner\n",
      "2021-10-15 10:59:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse,ner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 526/526 [18:34<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(combined_data.keys()):\n",
    "    combined_data[key]['stanza_tokens'] = nlp(combined_data[key]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the BIO tags\n",
    "We need to map the labels, that consist of character spans, to spans of tokens. Then we can tag the tokens with B-I-O labels.We do this per sentence, so we get a list of (token, tag) tuples per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(stanza_doc, labels, doc_key=''):\n",
    "    tokens = list(stanza_doc.iter_tokens())\n",
    "    char_to_tokenidx = np.repeat(None, len(stanza_doc.text))\n",
    "    for token_id, token in enumerate(tokens):\n",
    "        char_to_tokenidx[token.start_char:token.end_char] = token_id\n",
    "\n",
    "    new_label_list = {}\n",
    "    for i, (s,e,rel,text) in enumerate(labels):   \n",
    "        s_idx = char_to_tokenidx[s]\n",
    "        e_idx = char_to_tokenidx[e-1]\n",
    "        if s_idx is None:\n",
    "            # start index is a space so not inside token\n",
    "            print('{} - Warning: start index is space'.format(doc_key))\n",
    "            s_idx = char_to_tokenidx[s+1]\n",
    "        if e_idx is None:\n",
    "            # End index is space so not inside token\n",
    "            print('{} - Warning: end index is space'.format(doc_key))\n",
    "            e_idx = char_to_tokenidx[e-2]\n",
    "        subtext = stanza_doc.text[tokens[s_idx].start_char : tokens[e_idx].end_char]\n",
    "        if text.strip() != subtext:\n",
    "            print('{} - Warning: label \"{}\" does not match tokens \"{}\". Skipping this label'.format(\n",
    "                doc_key, text, subtext\n",
    "            ))\n",
    "        else:\n",
    "            new_label_list[i] = (s_idx, e_idx)\n",
    "    return tokens, new_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_BIO_tags(tokens, label_dict):\n",
    "    tags = np.repeat('O', len(tokens))\n",
    "    for i, (s,e) in label_dict.items():\n",
    "        tags[s] = 'B'\n",
    "        tags[s+1:e] = 'I'\n",
    "        \n",
    "    sent_dict = defaultdict(list)\n",
    "    for token_id, token in enumerate(tokens):\n",
    "        sent_dict[token.sent.id].append((token.text, tags[token_id]))\n",
    "    return [sent_dict[k] for k in sorted(sent_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1774 236 4-6 - Warning: label \"mitigate\" does not match tokens \"mitigated\". Skipping this label\n",
      "2484 335 6-3 - Warning: label \"to\" does not match tokens \"history\". Skipping this label\n",
      "2495 335 11-1 - Warning: label \"to\" does not match tokens \"Historically\". Skipping this label\n",
      "2499 335 15-2 - Warning: label \"for\" does not match tokens \"Before\". Skipping this label\n",
      "3552 546 3-1 - Warning: start index is space\n",
      "3559 546 5-4 - Warning: label \"n\" does not match tokens \"important\". Skipping this label\n",
      "1977 261 1-1 - Warning: label \"fuelled\" does not match tokens \"credit-fuelled\". Skipping this label\n",
      "1893 250 2-2 - Warning: label \"geared to strengthen\" does not match tokens \"geared to strengthening\". Skipping this label\n",
      "2597 351 6-1 - Warning: start index is space\n",
      "2413 320 3-5 - Warning: start index is space\n",
      "2413 320 3-5 - Warning: end index is space\n",
      "2414 320 4-3 - Warning: label \"if\" does not match tokens \"proliferation\". Skipping this label\n",
      "2790 385 3-5 - Warning: label \"if\" does not match tokens \"life\". Skipping this label\n",
      "1802 239 3-5 - Warning: label \"and accept severe risk\" does not match tokens \"and accept severe risks\". Skipping this label\n",
      "1860 247 5-1 - Warning: end index is space\n",
      "2991 433 1-5 - Warning: label \"to\" does not match tokens \"towards\". Skipping this label\n",
      "2993 433 3-3 - Warning: label \"reducing effects\" does not match tokens \"debt-reducing effects\". Skipping this label\n",
      "2896 418 2-2 - Warning: label \"To\" does not match tokens \"Today\". Skipping this label\n",
      "2115 283 2-5 - Warning: start index is space\n"
     ]
    }
   ],
   "source": [
    "for key in combined_data:\n",
    "    doc = combined_data[key]['stanza_tokens']\n",
    "    rel_labels = [l for l in combined_data[key]['label'] if l[2]=='Content_Relation_Explanation']\n",
    "    tokens, label_dict = map_labels(doc, rel_labels, doc_key=key)\n",
    "    combined_data[key]['tags'] = get_BIO_tags(tokens, label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some annotations seem to be incorrect, they do not match the tokenization. We skip those for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "We convert all of the stanza output to a dictionary and write that out to a file, so we don't have to run stanza again next time.\n",
    "We exclude the stanza document object because it's not json serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_key in combined_data:\n",
    "    doc = combined_data[doc_key]['stanza_tokens']\n",
    "    combined_data[doc_key]['stanza_output'] = doc.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = {doc_key: {k: v for k,v in data.items() if k!='stanza_tokens'}\n",
    "               for doc_key, data in combined_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/femke-parsed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as fout:\n",
    "    json.dump(reduced_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'r') as fin:\n",
    "    reduced_data = json.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the stanza output looks like for an example doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'text': 'Today',\n",
       "  'lemma': 'today',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 3,\n",
       "  'deprel': 'obl:tmod',\n",
       "  'start_char': 0,\n",
       "  'end_char': 5,\n",
       "  'ner': 'S-DATE'},\n",
       " {'id': 2,\n",
       "  'text': 'I',\n",
       "  'lemma': 'I',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'PRP',\n",
       "  'feats': 'Case=Nom|Number=Sing|Person=1|PronType=Prs',\n",
       "  'head': 3,\n",
       "  'deprel': 'nsubj',\n",
       "  'start_char': 6,\n",
       "  'end_char': 7,\n",
       "  'ner': 'O'},\n",
       " {'id': 3,\n",
       "  'text': 'want',\n",
       "  'lemma': 'want',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBP',\n",
       "  'feats': 'Mood=Ind|Tense=Pres|VerbForm=Fin',\n",
       "  'head': 0,\n",
       "  'deprel': 'root',\n",
       "  'start_char': 8,\n",
       "  'end_char': 12,\n",
       "  'ner': 'O'},\n",
       " {'id': 4,\n",
       "  'text': 'to',\n",
       "  'lemma': 'to',\n",
       "  'upos': 'PART',\n",
       "  'xpos': 'TO',\n",
       "  'head': 5,\n",
       "  'deprel': 'mark',\n",
       "  'start_char': 13,\n",
       "  'end_char': 15,\n",
       "  'ner': 'O'},\n",
       " {'id': 5,\n",
       "  'text': 'send',\n",
       "  'lemma': 'send',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VB',\n",
       "  'feats': 'VerbForm=Inf',\n",
       "  'head': 3,\n",
       "  'deprel': 'xcomp',\n",
       "  'start_char': 16,\n",
       "  'end_char': 20,\n",
       "  'ner': 'O'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_key = list(reduced_data.keys())[0]\n",
    "stanza_doc = reduced_data[example_key]['stanza_output']\n",
    "stanza_doc[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together train and test data\n",
    "We split up the paragraphs in sentences, and divide the collection of sentences in train and test. We select a limited number of features from the stanza output, and also include the features of surrounding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_features(sentence, context_window=1, features=['text', 'lemma', 'upos', 'xpos', 'deprel', 'ner']):\n",
    "    # Copy the exisiting features to the output list\n",
    "    output = [{feat: token[feat] for feat in features} for token in sentence]\n",
    "    \n",
    "    # Now include the context\n",
    "    for i in range(context_window, len(sentence)-context_window):\n",
    "        for feat in features:\n",
    "            for j in range(1, context_window+1):\n",
    "                name = 'min{}_{}'.format(j, feat)\n",
    "                output[i][name] = sentence[i-j][feat]\n",
    "                name = 'plus{}_{}'.format(j, feat)\n",
    "                output[i][name] = sentence[i+j][feat]\n",
    "                \n",
    "    # Also include special features for beginning and ending tokens\n",
    "    for i in range(context_window):\n",
    "        output[i]['BOS'] = True\n",
    "        output[len(output)-i-1]['EOS'] = True\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2605 2605\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_features = [ expand_features(sent)\n",
    "    for k,data in reduced_data.items()\n",
    "    for sent in data['stanza_output']\n",
    "]\n",
    "\n",
    "\n",
    "all_tags = [[tag for tok,tag in sent]\n",
    "    for k,data in reduced_data.items()\n",
    "    for sent in data['tags']\n",
    "]\n",
    "\n",
    "print(len(all_features), len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_features, all_tags, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2084 521\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea how balanced our dataset is, let's check how many of the sentences contain at least one B and/or I tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.345489443378119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_rel = [np.any([tag!='O' for tag in sent]) for sent in y_train]\n",
    "np.mean(contains_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually inspect the features of one sentence with a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>upos</th>\n",
       "      <th>xpos</th>\n",
       "      <th>deprel</th>\n",
       "      <th>ner</th>\n",
       "      <th>BOS</th>\n",
       "      <th>min1_text</th>\n",
       "      <th>plus1_text</th>\n",
       "      <th>min1_lemma</th>\n",
       "      <th>plus1_lemma</th>\n",
       "      <th>min1_upos</th>\n",
       "      <th>plus1_upos</th>\n",
       "      <th>min1_xpos</th>\n",
       "      <th>plus1_xpos</th>\n",
       "      <th>min1_deprel</th>\n",
       "      <th>plus1_deprel</th>\n",
       "      <th>min1_ner</th>\n",
       "      <th>plus1_ner</th>\n",
       "      <th>EOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They</td>\n",
       "      <td>they</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>took</td>\n",
       "      <td>take</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>root</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They</td>\n",
       "      <td>the</td>\n",
       "      <td>they</td>\n",
       "      <td>the</td>\n",
       "      <td>PRON</td>\n",
       "      <td>DET</td>\n",
       "      <td>PRP</td>\n",
       "      <td>DT</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>took</td>\n",
       "      <td>form</td>\n",
       "      <td>take</td>\n",
       "      <td>form</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VBD</td>\n",
       "      <td>NN</td>\n",
       "      <td>root</td>\n",
       "      <td>obj</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>form</td>\n",
       "      <td>form</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>obj</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>of</td>\n",
       "      <td>DET</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DT</td>\n",
       "      <td>IN</td>\n",
       "      <td>det</td>\n",
       "      <td>case</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>case</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>form</td>\n",
       "      <td>a</td>\n",
       "      <td>form</td>\n",
       "      <td>a</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "      <td>NN</td>\n",
       "      <td>DT</td>\n",
       "      <td>obj</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>prohibition</td>\n",
       "      <td>of</td>\n",
       "      <td>prohibition</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>IN</td>\n",
       "      <td>NN</td>\n",
       "      <td>case</td>\n",
       "      <td>nmod</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prohibition</td>\n",
       "      <td>prohibition</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nmod</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>of</td>\n",
       "      <td>a</td>\n",
       "      <td>of</td>\n",
       "      <td>DET</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DT</td>\n",
       "      <td>IN</td>\n",
       "      <td>det</td>\n",
       "      <td>case</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>case</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prohibition</td>\n",
       "      <td>monetary</td>\n",
       "      <td>prohibition</td>\n",
       "      <td>monetary</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>JJ</td>\n",
       "      <td>nmod</td>\n",
       "      <td>amod</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monetary</td>\n",
       "      <td>monetary</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>financing</td>\n",
       "      <td>of</td>\n",
       "      <td>financing</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>IN</td>\n",
       "      <td>NN</td>\n",
       "      <td>case</td>\n",
       "      <td>nmod</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>financing</td>\n",
       "      <td>financing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nmod</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>monetary</td>\n",
       "      <td>of</td>\n",
       "      <td>monetary</td>\n",
       "      <td>of</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADP</td>\n",
       "      <td>JJ</td>\n",
       "      <td>IN</td>\n",
       "      <td>amod</td>\n",
       "      <td>case</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>case</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>financing</td>\n",
       "      <td>government</td>\n",
       "      <td>financing</td>\n",
       "      <td>government</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nmod</td>\n",
       "      <td>compound</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>government</td>\n",
       "      <td>government</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>deficits</td>\n",
       "      <td>of</td>\n",
       "      <td>deficit</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>IN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>case</td>\n",
       "      <td>nmod</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deficits</td>\n",
       "      <td>deficit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>nmod</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>government</td>\n",
       "      <td>,</td>\n",
       "      <td>government</td>\n",
       "      <td>,</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NN</td>\n",
       "      <td>,</td>\n",
       "      <td>compound</td>\n",
       "      <td>punct</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deficits</td>\n",
       "      <td>the</td>\n",
       "      <td>deficit</td>\n",
       "      <td>the</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "      <td>NNS</td>\n",
       "      <td>DT</td>\n",
       "      <td>nmod</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,</td>\n",
       "      <td>no</td>\n",
       "      <td>,</td>\n",
       "      <td>no</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>DET</td>\n",
       "      <td>,</td>\n",
       "      <td>DT</td>\n",
       "      <td>punct</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>bail</td>\n",
       "      <td>the</td>\n",
       "      <td>bail</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>det</td>\n",
       "      <td>compound</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bail</td>\n",
       "      <td>bail</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>no</td>\n",
       "      <td>-</td>\n",
       "      <td>DET</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>DT</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>det</td>\n",
       "      <td>punct</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>punct</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bail</td>\n",
       "      <td>out</td>\n",
       "      <td>bail</td>\n",
       "      <td>out</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>compound</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>out</td>\n",
       "      <td>out</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>clause</td>\n",
       "      <td>-</td>\n",
       "      <td>clause</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>NN</td>\n",
       "      <td>punct</td>\n",
       "      <td>appos</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>clause</td>\n",
       "      <td>clause</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>appos</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>out</td>\n",
       "      <td>and</td>\n",
       "      <td>out</td>\n",
       "      <td>and</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>CC</td>\n",
       "      <td>compound</td>\n",
       "      <td>cc</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clause</td>\n",
       "      <td>the</td>\n",
       "      <td>clause</td>\n",
       "      <td>the</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "      <td>NN</td>\n",
       "      <td>DT</td>\n",
       "      <td>appos</td>\n",
       "      <td>det</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LAW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>B-LAW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>Stability</td>\n",
       "      <td>and</td>\n",
       "      <td>stability</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>CC</td>\n",
       "      <td>NN</td>\n",
       "      <td>cc</td>\n",
       "      <td>conj</td>\n",
       "      <td>O</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stability</td>\n",
       "      <td>stability</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>conj</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>and</td>\n",
       "      <td>the</td>\n",
       "      <td>and</td>\n",
       "      <td>DET</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>DT</td>\n",
       "      <td>CC</td>\n",
       "      <td>det</td>\n",
       "      <td>cc</td>\n",
       "      <td>B-LAW</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stability</td>\n",
       "      <td>Growth</td>\n",
       "      <td>stability</td>\n",
       "      <td>growth</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>conj</td>\n",
       "      <td>compound</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Growth</td>\n",
       "      <td>growth</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>Pact</td>\n",
       "      <td>and</td>\n",
       "      <td>pact</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>CC</td>\n",
       "      <td>NN</td>\n",
       "      <td>cc</td>\n",
       "      <td>conj</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>E-LAW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pact</td>\n",
       "      <td>pact</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>conj</td>\n",
       "      <td>E-LAW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Growth</td>\n",
       "      <td>.</td>\n",
       "      <td>growth</td>\n",
       "      <td>.</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NN</td>\n",
       "      <td>.</td>\n",
       "      <td>compound</td>\n",
       "      <td>punct</td>\n",
       "      <td>I-LAW</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text        lemma   upos  xpos    deprel    ner   BOS    min1_text  \\\n",
       "0          They         they   PRON   PRP     nsubj      O  True          NaN   \n",
       "1          took         take   VERB   VBD      root      O   NaN         They   \n",
       "2           the          the    DET    DT       det      O   NaN         took   \n",
       "3          form         form   NOUN    NN       obj      O   NaN          the   \n",
       "4            of           of    ADP    IN      case      O   NaN         form   \n",
       "5             a            a    DET    DT       det      O   NaN           of   \n",
       "6   prohibition  prohibition   NOUN    NN      nmod      O   NaN            a   \n",
       "7            of           of    ADP    IN      case      O   NaN  prohibition   \n",
       "8      monetary     monetary    ADJ    JJ      amod      O   NaN           of   \n",
       "9     financing    financing   NOUN    NN      nmod      O   NaN     monetary   \n",
       "10           of           of    ADP    IN      case      O   NaN    financing   \n",
       "11   government   government   NOUN    NN  compound      O   NaN           of   \n",
       "12     deficits      deficit   NOUN   NNS      nmod      O   NaN   government   \n",
       "13            ,            ,  PUNCT     ,     punct      O   NaN     deficits   \n",
       "14          the          the    DET    DT       det      O   NaN            ,   \n",
       "15           no           no    DET    DT       det      O   NaN          the   \n",
       "16         bail         bail   NOUN    NN  compound      O   NaN           no   \n",
       "17            -            -  PUNCT  HYPH     punct      O   NaN         bail   \n",
       "18          out          out   NOUN    NN  compound      O   NaN            -   \n",
       "19       clause       clause   NOUN    NN     appos      O   NaN          out   \n",
       "20          and          and  CCONJ    CC        cc      O   NaN       clause   \n",
       "21          the          the    DET    DT       det  B-LAW   NaN          and   \n",
       "22    Stability    stability   NOUN    NN      conj  I-LAW   NaN          the   \n",
       "23          and          and  CCONJ    CC        cc  I-LAW   NaN    Stability   \n",
       "24       Growth       growth   NOUN    NN  compound  I-LAW   NaN          and   \n",
       "25         Pact         pact   NOUN    NN      conj  E-LAW   NaN       Growth   \n",
       "26            .            .  PUNCT     .     punct      O   NaN          NaN   \n",
       "\n",
       "     plus1_text   min1_lemma  plus1_lemma min1_upos plus1_upos min1_xpos  \\\n",
       "0           NaN          NaN          NaN       NaN        NaN       NaN   \n",
       "1           the         they          the      PRON        DET       PRP   \n",
       "2          form         take         form      VERB       NOUN       VBD   \n",
       "3            of          the           of       DET        ADP        DT   \n",
       "4             a         form            a      NOUN        DET        NN   \n",
       "5   prohibition           of  prohibition       ADP       NOUN        IN   \n",
       "6            of            a           of       DET        ADP        DT   \n",
       "7      monetary  prohibition     monetary      NOUN        ADJ        NN   \n",
       "8     financing           of    financing       ADP       NOUN        IN   \n",
       "9            of     monetary           of       ADJ        ADP        JJ   \n",
       "10   government    financing   government      NOUN       NOUN        NN   \n",
       "11     deficits           of      deficit       ADP       NOUN        IN   \n",
       "12            ,   government            ,      NOUN      PUNCT        NN   \n",
       "13          the      deficit          the      NOUN        DET       NNS   \n",
       "14           no            ,           no     PUNCT        DET         ,   \n",
       "15         bail          the         bail       DET       NOUN        DT   \n",
       "16            -           no            -       DET      PUNCT        DT   \n",
       "17          out         bail          out      NOUN       NOUN        NN   \n",
       "18       clause            -       clause     PUNCT       NOUN      HYPH   \n",
       "19          and          out          and      NOUN      CCONJ        NN   \n",
       "20          the       clause          the      NOUN        DET        NN   \n",
       "21    Stability          and    stability     CCONJ       NOUN        CC   \n",
       "22          and          the          and       DET      CCONJ        DT   \n",
       "23       Growth    stability       growth      NOUN       NOUN        NN   \n",
       "24         Pact          and         pact     CCONJ       NOUN        CC   \n",
       "25            .       growth            .      NOUN      PUNCT        NN   \n",
       "26          NaN          NaN          NaN       NaN        NaN       NaN   \n",
       "\n",
       "   plus1_xpos min1_deprel plus1_deprel min1_ner plus1_ner   EOS  \n",
       "0         NaN         NaN          NaN      NaN       NaN   NaN  \n",
       "1          DT       nsubj          det        O         O   NaN  \n",
       "2          NN        root          obj        O         O   NaN  \n",
       "3          IN         det         case        O         O   NaN  \n",
       "4          DT         obj          det        O         O   NaN  \n",
       "5          NN        case         nmod        O         O   NaN  \n",
       "6          IN         det         case        O         O   NaN  \n",
       "7          JJ        nmod         amod        O         O   NaN  \n",
       "8          NN        case         nmod        O         O   NaN  \n",
       "9          IN        amod         case        O         O   NaN  \n",
       "10         NN        nmod     compound        O         O   NaN  \n",
       "11        NNS        case         nmod        O         O   NaN  \n",
       "12          ,    compound        punct        O         O   NaN  \n",
       "13         DT        nmod          det        O         O   NaN  \n",
       "14         DT       punct          det        O         O   NaN  \n",
       "15         NN         det     compound        O         O   NaN  \n",
       "16       HYPH         det        punct        O         O   NaN  \n",
       "17         NN    compound     compound        O         O   NaN  \n",
       "18         NN       punct        appos        O         O   NaN  \n",
       "19         CC    compound           cc        O         O   NaN  \n",
       "20         DT       appos          det        O     B-LAW   NaN  \n",
       "21         NN          cc         conj        O     I-LAW   NaN  \n",
       "22         CC         det           cc    B-LAW     I-LAW   NaN  \n",
       "23         NN        conj     compound    I-LAW     I-LAW   NaN  \n",
       "24         NN          cc         conj    I-LAW     E-LAW   NaN  \n",
       "25          .    compound        punct    I-LAW         O   NaN  \n",
       "26        NaN         NaN          NaN      NaN       NaN  True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CRF model\n",
    "We use the sklearn-crfsuite package, which can take arbitrary categorical features as input. See [this tutorial](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: six in /home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages (from sklearn-crfsuite) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages (from sklearn-crfsuite) (4.62.3)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tabulate, python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6 tabulate-0.8.9\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, sklearn-crfsuite does not work with recent scikit-learn versions (https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) so we have to downgrade the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn<0.24\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 1.8 MB/s eta 0:00:01     |████▉                           | 1.0 MB 3.8 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages (from scikit-learn<0.24) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages (from scikit-learn<0.24) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages (from scikit-learn<0.24) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages (from scikit-learn<0.24) (3.0.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "Successfully installed scikit-learn-0.23.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U 'scikit-learn<0.24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We can calculate metrics for all labels, but since there are many O's, we can also calculate only for B and I tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite.metrics\n",
    "\n",
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B      0.338     0.121     0.178       215\n",
      "           I      0.320     0.121     0.175       199\n",
      "           O      0.970     0.992     0.981     11763\n",
      "\n",
      "    accuracy                          0.962     12177\n",
      "   macro avg      0.543     0.411     0.445     12177\n",
      "weighted avg      0.948     0.962     0.953     12177\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda3/envs/ssi-cm/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(sklearn_crfsuite.metrics.flat_classification_report(\n",
    "    y_test, y_pred, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17668836981036198"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['B', 'I']\n",
    "sklearn_crfsuite.metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7958975583021689"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to F1 on training set, to see wheter we are overfitting\n",
    "y_train_pred = crf.predict(X_train)\n",
    "sklearn_crfsuite.metrics.flat_f1_score(y_train, y_train_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To the extent that higher market interest rates in the euro area lead to pressure on the krone , Danmarks Nationalbank will follow its usual practice , including raising interest rates if necessary .\n",
      "Predicted: lead\n",
      "True: lead\n",
      "\n",
      "It should be fairer so that everyone contributes to the welfare system according to their means .\n",
      "Predicted: contributes\n",
      "True: so\n",
      "\n",
      "I believe that the key to this mystery lies in the fact that the euro represents a commitment device , a policy straitjacket which , if accompanied by the behaviour to which the country implicitly and explicitly commits when it joins the currency union , will lead to an improved economic performance relative to what can otherwise be achieved .\n",
      "Predicted: will lead\n",
      "True: will lead\n",
      "\n",
      "In combination with high current account deficits , resulting from lack of competitiveness , this leaves these countries especially vulnerable to changes of sentiment in the international capital markets ( slide 4 - current account ) .\n",
      "Predicted: resulting\n",
      "True: resulting leaves\n",
      "\n",
      "Domestically , we are seeing tax cuts which , together with low interest rates , will boost the purchasing power of private households in 2010 .\n",
      "Predicted: will\n",
      "True: will\n",
      "\n",
      "These measures - notably the two European stability mechanisms , the EFSFand the ESM - stabilised the euro area in the short term by offering financial assistance in exchange for structural reforms .\n",
      "Predicted: stabilised by\n",
      "True: stabilised\n",
      "\n",
      "At the same time , housing bubbles emerged in several member states , and this later caused distress for the banks that had helped to fund property projects at inflated prices .\n",
      "Predicted: this later\n",
      "True: \n",
      "\n",
      "People worry that a stagnating economy will lead to the emergence of a \" lost generation \" , where young people have no jobs , and even worse , no hope .\n",
      "Predicted: will lead\n",
      "True: will lead\n",
      "\n",
      "If we want to boost employment in these areas , we must improve competitiveness - there are no other options .\n",
      "Predicted: If we want to\n",
      "True: \n",
      "\n",
      "The result has been that our economies are not flexible enough to tackle the challenges of monetary union , and that there remain important weaknesses in its design which need to be corrected .\n",
      "Predicted: which need\n",
      "True: The to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out some example sentences where the CRF did find relations\n",
    "num_example = 10\n",
    "i = 0\n",
    "for sent, pred_tags, true_tags in zip(X_test, y_pred, y_test):\n",
    "    if 'B' in pred_tags or 'I' in pred_tags:\n",
    "        if i < num_example:\n",
    "            print(' '.join([tok['text'] for tok in sent]))\n",
    "            print('Predicted:', ' '.join(tok['text'] for tok,tag in zip(sent, pred_tags) if tag!='O'))\n",
    "            print('True:', ' '.join(tok['text'] for tok,tag in zip(sent, true_tags) if tag!='O'))\n",
    "            print()\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ssi-cm]",
   "language": "python",
   "name": "conda-env-ssi-cm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
