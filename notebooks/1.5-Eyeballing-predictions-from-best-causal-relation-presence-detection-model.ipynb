{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyeballing predictions\n",
    "To directly demonstrate our best model results so far, we will provide a small prediction dataset for people (especially for non-technical side) to analyze and review. \n",
    "\n",
    "### Methodology\n",
    "In this experiment, we will use the model of Fine-tuning bert classifier in 1.3-causal-relation-presence-bert-for-sequence-classification. The model architecture of Fine-Tuning Bert is as follows, and this figure is referenced from: https://mccormickml.com/2019/07/22/BERT-fine-tuning/.\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" style=\"width:600px;height:500px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\PythonJupyterStudy\\CM\\CM_Macro\\SSIML2021\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
    "from src.data.make_dataset import read_data_file,make_dataset\n",
    "from src.util import get_device,over_sampling,transform_df,plot_confusion_matrix,format_time,train_val_test\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import BertTokenizer,RobertaTokenizer,BertForSequenceClassification,get_linear_schedule_with_warmup,AdamW, BertConfig, RobertaForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "#import wandb\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\"csv\"), 'The directory \"csv\" does not exist!'\n",
    "assert os.path.isdir(\"txt\"), 'The directory \"txt\" does not exist!'\n",
    "map_contents = read_data_file(\"csv/Map_Contents-20200726.csv\")\n",
    "speech_contents = read_data_file(\"csv/Speech_Contents-20210520.txt\")\n",
    "speeches = read_data_file(\"csv/Speeches-20210520.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mwarning: mismatch meta data (10) vs file (7) for file 1998-12-14 Schroeder ann.txt\u001b[0m\n",
      "skipping file in language fr: 2009-12-01 Sarkozy Elysee (Economy) ann fr.txt\n",
      "skipping file in language fr: 2009-12-14 Sarkozy Elysee (Economy) ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (2) vs file (1) for file 2010-03-29 Brown sfm2020 ann.txt\u001b[0m\n",
      "skipping file in language fr: 2010-04-20 Barroso European Commission ann fr.txt\n",
      "skipping file in language fr: 2011-01-13 Sarkozy gb ann.txt\n",
      "skipping file in language nl: 2011-04-06 Rutte FD evenement ann NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (12) for file 2011-05-20 Weidmann Deutsche Bundesbank_01 ANN.txt\u001b[0m\n",
      "skipping file in language nl: 2011-09-27 Rutte Rijksoverheid ann.txt\n",
      "skipping file in language nl: 2011-10-28 Knot dnb_01 ANN NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-10 Orban London ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-12 Rutte Trilateral Commission - code ann.txt\u001b[0m\n",
      "skipping file in language de: 2012-01-06 Rutte CSU klausurtagung ann G.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 208; file name: 2012-02-24 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (24) vs file (22) for file 2012-07-11 Rajoy La Moncloa ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (11) vs file (10) for file 2012-07-17 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "skipping file in language unk: 2012-07-26 Barroso European Commission.txt\n",
      "skipping file in language fr: 2012-08-30 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 335; file name: 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (7) vs file (6) for file 2012-12-07 Simor Bis.org ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-02-19 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (19) vs file (18) for file 2013-02-26 Rohde Danmarks Nationalbank ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-04-17 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (13) for file 2013-05-31 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (22) vs file (19) for file 2013-10-08 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2013-11-21 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language de: 2014-02-27 Merkel Bundesregerung ann g.txt\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2014-07-18 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2015-01-19 Merkel Bundesregerung ann g.txt\n",
      "skipping file placeholder.txt\n",
      "read 118 files with 897 paragraphs; skipped 16 files\n",
      "Preprocessing:\n",
      "\n",
      "19 na data found\n",
      "na data dropped\n"
     ]
    }
   ],
   "source": [
    "X, y = make_dataset(speeches, speech_contents, map_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "# get divice info for torch\n",
    "device = get_device()\n",
    "\n",
    "# set seed for random, numpy, torch\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "if str(device) == 'cuda':\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "else:\n",
    "    torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_process(df,tokenizer,max_length):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    sentences = df.X.values\n",
    "    labels = df.y.values\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,   # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    #print('Check the original paragraph and converted paragrapg: ')\n",
    "    #print('Original: ', sentences[1])\n",
    "    #print('Token IDs:', input_ids[1])\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return dataset\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps):\n",
    "    print('\\nTraining and evaluating the model.')\n",
    "    \n",
    "    #store a number of quantities such as training and validation loss,validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    #store prediction and true labels\n",
    "    train_logits = []\n",
    "    train_label = []\n",
    "    best_loss = 1.0\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "    \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "    \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "    \n",
    "            model.zero_grad()        \n",
    "            loss, logits = model(b_input_ids, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=b_input_mask, \n",
    "                                 labels=b_labels)\n",
    "            \n",
    "            logits_ = logits.detach().cpu().numpy()\n",
    "            label_ids_ = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            #store prediction for the last epoch\n",
    "            if epoch_i == epochs-1:\n",
    "                train_logits.extend(logits_)\n",
    "                train_label.extend(label_ids_)\n",
    "            total_train_loss += loss.item()\n",
    "    \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "            # Update parameters and take a step using the computed gradient.Update the learning rate.\n",
    "            optimizer.step()\n",
    "        \n",
    "        #Update scheduler(lr decay) every epoch\n",
    "        lr_stat_opt = optimizer.param_groups[0][\"lr\"] #or lr_stat_scheduler = scheduler.get_last_lr()[0]\n",
    "        print('current lr is:',lr_stat_opt)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        #wandb.log({\"lr\": lr_stat_opt})\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        eval_pred = []\n",
    "        eval_label = []\n",
    "        t0 = time.time()\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "            # `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                (loss, logits) = model(b_input_ids, \n",
    "                                       token_type_ids=None, \n",
    "                                       attention_mask=b_input_mask,\n",
    "                                       labels=b_labels)\n",
    "                \n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            #total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            eval_pred.extend(logits)\n",
    "            eval_label.extend(label_ids)\n",
    "        \n",
    "\n",
    "        eval_pred = convert_logits_tolabel(eval_pred)\n",
    "        f1_val = f1_score(eval_label,eval_pred,average='macro')\n",
    "        \n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        #avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Macro F1 score: {0:.2f}\".format(f1_val))\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "        \n",
    "        \n",
    "        #early-stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            # At this point also save a snapshot of the current model\n",
    "            torch.save(model, 'my_model_best_loss.pth')\n",
    "            \n",
    "        \n",
    "        #wandb.log({\"Training Loss\": avg_train_loss, \"Valid. Loss\":avg_val_loss,\"Valid. Macro F1\":f1_val, \"epoch\": epoch_i + 1 })\n",
    "        \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Macro F1.': f1_val,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    pd.set_option('precision', 2)\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    \n",
    "    # Plot the learning curve.\n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "    \n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    epoch_list = [i+1 for i in range(epochs)]\n",
    "    plt.xticks(epoch_list)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    train_pred = convert_logits_tolabel(train_logits)\n",
    "    \n",
    "    return train_pred,eval_pred,train_label,eval_label\n",
    "\n",
    "def get_prediction(df_test,model,batch_size,max_length,model_name):\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "\n",
    "    test_dataset = tokenize_process(df_test,tokenizer,max_length)\n",
    "    test_dataloader = DataLoader(test_dataset,sampler = SequentialSampler(test_dataset),batch_size = batch_size)\n",
    "    \n",
    "    test_logits = []\n",
    "    test_label = []\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        \"\"\"\n",
    "        Unpack this training batch from our dataloader.\n",
    "        `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "        \"\"\"\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].long().to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            test_logits.extend(logits)\n",
    "            test_label.extend(label_ids)\n",
    "            \n",
    "    def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred\n",
    "    \n",
    "    test_logits = convert_logits_tolabel(test_logits)\n",
    "            \n",
    "    return test_logits,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_cls(df_train,df_val,\n",
    "             epochs = 10,\n",
    "             batch_size =16,\n",
    "             max_length=128,\n",
    "             model_name='roberta',\n",
    "             lr = 5e-5,\n",
    "             weight_decay = 1e-2,\n",
    "             freeze_layer_count=1,\n",
    "             scheduler_type='step',\n",
    "             decayRate=0.75):\n",
    "    \n",
    "    \n",
    "    #wandb update\n",
    "    #wandb.config.max_length = max_length\n",
    "    #wandb.config.model = model_name\n",
    "    #wandb.config.weight_decay = weight_decay\n",
    "    #wandb.config.freeze_layer_count = freeze_layer_count\n",
    "    #wandb.config.optimizer = 'AdamW'\n",
    "    #wandb.config.scheduler_type = scheduler_type\n",
    "    #wandb.config.decayRate = decayRate\n",
    "    \n",
    "    \n",
    "    print('\\n======================Doing Bert classification task======================')\n",
    "    \n",
    "    \"\"\"\n",
    "    step1: Tokenization\n",
    "    \"\"\"\n",
    "    #print('Do step1: Tokenization\\n')\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "    \n",
    "    train_dataset = tokenize_process(df_train,tokenizer,max_length)\n",
    "    val_dataset = tokenize_process(df_val,tokenizer,max_length)\n",
    "    \n",
    "    \"\"\"\n",
    "    step2: create dataloader for both training and eval set\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = batch_size\n",
    "    \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "    \n",
    "    # For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    step3: load bert model\n",
    "    \"\"\"\n",
    "    #print('Do step3: load bert model\\n')\n",
    "    model,optimizer,scheduler,total_steps = model_and_helper(epochs,train_dataloader,model_name,freeze_layer_count,lr,weight_decay,scheduler_type,decayRate)\n",
    "    \n",
    "    \"\"\"\n",
    "    step4: Training and evaluating\n",
    "    \"\"\"\n",
    "    #print('Do step4: Training and evaluating\\n')\n",
    "    train_pred,eval_pred,train_label,eval_label = Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps)\n",
    "    \n",
    "    \n",
    "    return train_pred,eval_pred,train_label,eval_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_and_helper(epochs,train_dataloader,\n",
    "                     model_name='roberta',\n",
    "                     freeze_layer_count=1,\n",
    "                     lr = 5e-5,\n",
    "                     weight_decay = 1e-2,\n",
    "                     scheduler_type='linear',\n",
    "                     decayRate=0.75):\n",
    "    \n",
    "    if model_name == 'bert':\n",
    "        print('\\nLoading bert model.')\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\",  num_labels = 2,  output_attentions = False, output_hidden_states = False,  return_dict = False)\n",
    "        \n",
    "        \n",
    "    elif model_name == 'roberta':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            'roberta-base', num_labels = 2, output_attentions = False, output_hidden_states = False, return_dict = False)\n",
    "        \n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Assign GPU if you have\n",
    "    if str(device)=='cuda':\n",
    "        model.cuda()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # We freeze here the embeddings of the model\n",
    "    if freeze_layer_count:\n",
    "        if model_name == 'bert':\n",
    "            for param in model.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.bert.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "        if model_name == 'roberta':\n",
    "            for param in model.roberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.roberta.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "        \n",
    "    \n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch), the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = lr, \n",
    "                      eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                      weight_decay = weight_decay \n",
    "                    )\n",
    "    \n",
    "    epochs = epochs\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # set learning rate decay\n",
    "    if scheduler_type=='linear': \n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value\n",
    "                                                num_training_steps = total_steps)\n",
    "    elif scheduler_type=='step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='exponential':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='none':\n",
    "        scheduler = None\n",
    "    \n",
    "    else:\n",
    "        raise SystemExit('Invalid scheduler_type, it could only be one of [linear, step, exponential,none] ')\n",
    "    \n",
    "\n",
    "    return model,optimizer,scheduler,total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args):\n",
    "    \n",
    "    training_pred,training_true,evaluation_pred,evaluation_true = [],[],[],[]\n",
    "    \n",
    "    training_pred.append(train_pred)\n",
    "    training_true.append(train_label)\n",
    "    evaluation_pred.append(eval_pred)\n",
    "    evaluation_true.append(eval_label)\n",
    "    \n",
    "    def flatten(t):\n",
    "        return [item for sublist in t for item in sublist]\n",
    "    \n",
    "    evaluation_true = flatten(evaluation_true)\n",
    "    evaluation_pred = flatten(evaluation_pred)\n",
    "    training_true = flatten(training_true)\n",
    "    training_pred = flatten(training_pred)\n",
    "    \n",
    "    \n",
    "    f1_train = f1_score(training_true, training_pred,average='macro')\n",
    "    f1_val = f1_score(evaluation_true, evaluation_pred,average='macro')\n",
    "    f1_test = f1_score(test_true,test_pred,average='macro')\n",
    "    \n",
    "    print({\"f1_train\": f1_train,\"f1_val\": f1_val, \"f1_test\": f1_test})\n",
    "    #wandb.log({\"f1_train\": f1_train,\"f1_val\": f1_val, \"f1_test\": f1_test})\n",
    "\n",
    "    print('\\nargs:',args)\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    #For evaluation data\n",
    "    print('classification report on test set is:\\n')\n",
    "    clas_reprt_eval = classification_report(test_true, test_pred, target_names=target_names)\n",
    "    print(clas_reprt_eval)\n",
    "    \n",
    "    print('confusion matrix on test set is:\\n')\n",
    "    cm_eval = confusion_matrix(test_true, test_pred)\n",
    "    plot_confusion_matrix(cm_eval, ['No causal relation', 'Has causal relation'], normalize=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eyeballing_prediction(X,y,args):\n",
    "    \n",
    "    # we will treat test set as our eyeballing dataset (n=100)\n",
    "    df_train, df_val, df_test = train_val_test(args['train_ratio'], args['validation_ratio'], args['test_ratio'],X,y,seed_val)\n",
    "    \n",
    "    \n",
    "    print('In this run, we have {} data points for training set.'.format(len(df_train)))\n",
    "    # over-sample the training set\n",
    "    df_train = over_sampling(df_train,seed_val)\n",
    "    \n",
    "    train_pred,eval_pred,train_label,eval_label = transformer_cls(df_train,df_val,\n",
    "                                                                          epochs = args['epochs'],\n",
    "                                                                          batch_size =args['batch_size'],\n",
    "                                                                          max_length=args['max_length'],\n",
    "                                                                          model_name=args['model_name'],\n",
    "                                                                          lr = args['lr'],\n",
    "                                                                          weight_decay = args['weight_decay'],\n",
    "                                                                          freeze_layer_count=args['freeze_layer_count'],\n",
    "                                                                          scheduler_type=args['scheduler_type'],\n",
    "                                                                          decayRate=args['decayRate'])\n",
    "    \n",
    "    #load best model\n",
    "    model = torch.load('my_model_best_loss.pth')\n",
    "    model.eval()\n",
    "    \n",
    "    test_pred,test_true = get_prediction(df_test,model,batch_size=args['batch_size'],max_length=args['max_length'],model_name=args['model_name'])\n",
    "    \n",
    "    # generate confusion matrix and classification report\n",
    "    evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args)\n",
    "\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #clear output if you want\n",
    "    #clear_output(wait=True)\n",
    "    return test_pred,test_true,df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_args = {\n",
    "    'epochs': 14,\n",
    "    'batch_size': 20,\n",
    "    'train_ratio': 0.75,\n",
    "    'validation_ratio': 0.137,\n",
    "    'test_ratio': 0.113, #generate 100 data points\n",
    "    'max_length': 128,\n",
    "    'model_name': 'roberta',\n",
    "    'lr': 1e-05,\n",
    "    'weight_decay': 0.05,\n",
    "     'freeze_layer_count': 6,\n",
    "    'scheduler_type': 'step',\n",
    "    'decayRate': 0.75\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X training set shape, X validation set shape, X test set shape]: (658,) (120,) (100,)\n",
      "In this run, we have 658 data points for training set.\n",
      "\n",
      "We totally have 956 training data after oversampling.\n",
      "\n",
      "======================Doing Bert classification task======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating the model.\n",
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:07.\n",
      "current lr is: 1e-05\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:00:08\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.24\n",
      "  Validation Loss: 0.72\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 7.500000000000001e-06\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epcoh took: 0:00:08\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.67\n",
      "  Validation Loss: 0.58\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 5.625000000000001e-06\n",
      "\n",
      "  Average training loss: 0.61\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.70\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 4.218750000000001e-06\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.73\n",
      "  Validation Loss: 0.56\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 3.1640625000000007e-06\n",
      "\n",
      "  Average training loss: 0.48\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.70\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 2.3730468750000007e-06\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.73\n",
      "  Validation Loss: 0.50\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 1.7797851562500005e-06\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.75\n",
      "  Validation Loss: 0.51\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 1.3348388671875005e-06\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.74\n",
      "  Validation Loss: 0.52\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 1.0011291503906254e-06\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.71\n",
      "  Validation Loss: 0.55\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 7.508468627929691e-07\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.73\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 5.631351470947268e-07\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.75\n",
      "  Validation Loss: 0.53\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 4.223513603210451e-07\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.74\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 3.1676352024078383e-07\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.73\n",
      "  Validation Loss: 0.55\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 2.3757264018058787e-07\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epcoh took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.73\n",
      "  Validation Loss: 0.56\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:53 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_train': 0.871335392614817, 'f1_val': 0.7297926857675285, 'f1_test': 0.6684749849306811}\n",
      "\n",
      "args: {'epochs': 14, 'batch_size': 20, 'train_ratio': 0.75, 'validation_ratio': 0.137, 'test_ratio': 0.113, 'max_length': 128, 'model_name': 'roberta', 'lr': 1e-05, 'weight_decay': 0.05, 'freeze_layer_count': 6, 'scheduler_type': 'step', 'decayRate': 0.75}\n",
      "classification report on test set is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.45      0.50      0.48        20\n",
      "     class 1       0.87      0.85      0.86        80\n",
      "\n",
      "    accuracy                           0.78       100\n",
      "   macro avg       0.66      0.68      0.67       100\n",
      "weighted avg       0.79      0.78      0.78       100\n",
      "\n",
      "confusion matrix on test set is:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pred,test_true,df_test = eyeballing_prediction(X,y,best_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prediction to df_test, then save it as csv file\n",
    "\n",
    "df_test['prediction'] = test_pred\n",
    "df_test.columns = ['text', 'label', 'prediction']\n",
    "\n",
    "# To keep consistency, we will use df to relocate those data points in df_test,\n",
    "# because some data points in df_test might be truncated \n",
    "df = pd.DataFrame({'X': pd.Series(X), 'y': pd.Series(y)})\n",
    "eyeballing = df.loc[list(df_test.index)]\n",
    "\n",
    "eyeballing['prediction'] = test_pred\n",
    "eyeballing.columns = ['text', 'label', 'prediction']\n",
    "\n",
    "eyeballing.loc[eyeballing['label'] == True, 'label'] = 1\n",
    "eyeballing.loc[eyeballing['label'] == False, 'label'] = 0\n",
    "eyeballing['label'] = eyeballing['label'].astype(int)\n",
    "\n",
    "# add Speech_ID\n",
    "Speech_ID = []\n",
    "paragraph = []\n",
    "for i in list(eyeballing.index):\n",
    "    Speech_ID.append(i.split(' ')[0])\n",
    "    paragraph.append(i.split(' ')[1])\n",
    "    \n",
    "eyeballing['source'] = Speech_ID\n",
    "eyeballing['paragraph'] = paragraph\n",
    "eyeballing['Speech_ID'] = None\n",
    "\n",
    "df_source = speeches[speeches['Speech_ID'].isin(sources)]\n",
    "\n",
    "for i in df_source['Speech_ID'].values:\n",
    "    eyeballing.loc[eyeballing[eyeballing['source']==str(i)].index, \"Speech_ID\"] =df_source[df_source['Speech_ID'] == i]['Speech_Identifier'].values[0]\n",
    "\n",
    "eyeballing = eyeballing[['Speech_ID', 'text', 'label', 'prediction', 'source', 'paragraph']]\n",
    "eyeballing = eyeballing.drop(columns=['source', 'paragraph'])\n",
    "eyeballing.to_excel(\"Eyeballing-predictions.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
