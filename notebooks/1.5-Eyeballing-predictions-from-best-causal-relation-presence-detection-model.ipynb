{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyeballing predictions\n",
    "To directly demonstrate our best model results so far, we will provide a small prediction dataset for people (especially for non-technical side) to analyze and review. \n",
    "\n",
    "### Methodology\n",
    "In this experiment, we will use the model of Fine-tuning bert classifier in 1.3-causal-relation-presence-bert-for-sequence-classification. The model architecture of Fine-Tuning Bert is as follows, and this figure is referenced from: https://mccormickml.com/2019/07/22/BERT-fine-tuning/.\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" style=\"width:600px;height:500px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\gitlab\\SSIML2021\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
    "from src.data.make_dataset import read_data_file,make_dataset\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import BertTokenizer,RobertaTokenizer,BertForSequenceClassification,get_linear_schedule_with_warmup,AdamW, BertConfig, RobertaForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle,resample\n",
    "#import wandb\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\"data/annotations\"), 'The directory \"data/annotations\" does not exist!'\n",
    "assert os.path.isdir(\"data/documents\"), 'The directory \"data/documents\" does not exist!'\n",
    "map_contents = read_data_file(\"data/annotations/Map_Contents-20200726.csv\")\n",
    "speech_contents = read_data_file(\"data/annotations/Speech_Contents-20210520.txt\")\n",
    "speeches = read_data_file(\"data/annotations/Speeches-20210520.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mwarning: mismatch meta data (10) vs file (7) for file data\\documents\\1998-12-14 Schroeder ann.txt\u001b[0m\n",
      "skipping file in language fr: data\\documents\\2009-12-01 Sarkozy Elysee (Economy) ann fr.txt\n",
      "skipping file in language fr: data\\documents\\2009-12-14 Sarkozy Elysee (Economy) ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (2) vs file (1) for file data\\documents\\2010-03-29 Brown sfm2020 ann.txt\u001b[0m\n",
      "skipping file in language fr: data\\documents\\2010-04-20 Barroso European Commission ann fr.txt\n",
      "skipping file in language fr: data\\documents\\2011-01-13 Sarkozy gb ann.txt\n",
      "skipping file in language nl: data\\documents\\2011-04-06 Rutte FD evenement ann NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (12) for file data\\documents\\2011-05-20 Weidmann Deutsche Bundesbank_01 ANN.txt\u001b[0m\n",
      "skipping file in language nl: data\\documents\\2011-09-27 Rutte Rijksoverheid ann.txt\n",
      "skipping file in language nl: data\\documents\\2011-10-28 Knot dnb_01 ANN NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file data\\documents\\2011-11-10 Orban London ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file data\\documents\\2011-11-12 Rutte Trilateral Commission - code ann.txt\u001b[0m\n",
      "skipping file in language de: data\\documents\\2012-01-06 Rutte CSU klausurtagung ann G.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 208; file name: data\\documents\\2012-02-24 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (24) vs file (22) for file data\\documents\\2012-07-11 Rajoy La Moncloa ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (11) vs file (10) for file data\\documents\\2012-07-17 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "skipping file in language unk: data\\documents\\2012-07-26 Barroso European Commission.txt\n",
      "skipping file in language fr: data\\documents\\2012-08-30 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 335; file name: data\\documents\\2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file data\\documents\\2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (7) vs file (6) for file data\\documents\\2012-12-07 Simor Bis.org ann.txt\u001b[0m\n",
      "skipping file in language fr: data\\documents\\2013-02-19 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (19) vs file (18) for file data\\documents\\2013-02-26 Rohde Danmarks Nationalbank ann.txt\u001b[0m\n",
      "skipping file in language fr: data\\documents\\2013-04-17 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (13) for file data\\documents\\2013-05-31 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (22) vs file (19) for file data\\documents\\2013-10-08 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: data\\documents\\2013-11-21 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language de: data\\documents\\2014-02-27 Merkel Bundesregerung ann g.txt\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file data\\documents\\2014-07-18 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: data\\documents\\2015-01-19 Merkel Bundesregerung ann g.txt\n",
      "skipping file data\\documents\\placeholder.txt\n",
      "read 118 files with 897 paragraphs; skipped 16 files\n"
     ]
    }
   ],
   "source": [
    "X, y = make_dataset(speeches, speech_contents, map_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "# get divice info for torch\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        # Tell PyTorch to use the GPU.\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# set seed for random, numpy, torch\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "if str(device) == 'cuda':\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "else:\n",
    "    torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(df,seed_val):\n",
    "    \"\"\"\n",
    "    remove Missing value first, then output two balanced dataset (Undersampling and Oversampling)\n",
    "    Input: X,y before pre-processing\n",
    "    Output: dataframes after removing missing value, Undersampling and Oversampling\n",
    "    \"\"\"\n",
    "    df_true = df[df['y'] == True]\n",
    "    df_false = df[df['y'] == False]\n",
    "\n",
    "    # Upsampling, for the class with less data, copy some data\n",
    "    df_false_upsampled = resample(df_false, random_state=seed_val, n_samples=len(df_true), replace=True)\n",
    "    df_upsampled = pd.concat([df_false_upsampled, df_true])\n",
    "    df_upsampled = shuffle(df_upsampled)\n",
    "\n",
    "    print('\\nWe totally have {} training data after oversampling.'.format(len(df_upsampled)))\n",
    "    return df_upsampled\n",
    "\n",
    "def transform_df(df):\n",
    "    # transform label to int\n",
    "    df.loc[df['y'] == 'True', 'y'] = 1\n",
    "    df.loc[df['y'] == 'False', 'y'] = 0\n",
    "    df.y = df.y.astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def train_val_test(train_ratio, validation_ratio, test_ratio, X, y,seed_val):\n",
    "    \"\"\"\n",
    "    remove Missing value first, then output two balanced dataset (Undersampling and Oversampling)\n",
    "    Input: X,y before pre-processing\n",
    "    Output: dataframes after removing missing value, Undersampling and Oversampling\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame({'X': pd.Series(X), 'y': pd.Series(y)})\n",
    "    df = df.dropna()\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=1 - train_ratio, random_state=seed_val)\n",
    "    df_val, df_test = train_test_split(df_test, test_size=test_ratio / (test_ratio + validation_ratio),\n",
    "                                       random_state=seed_val)\n",
    "\n",
    "    X_train, y_train = df_train['X'], df_train['y']\n",
    "    X_val, y_val = df_val['X'], df_val['y']\n",
    "    X_test, y_test = df_test['X'], df_test['y']\n",
    "\n",
    "    print('[X training set shape, X validation set shape, X test set shape]:', y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "    df_train = pd.DataFrame({'X': pd.Series(X_train), 'y': pd.Series(y_train)})\n",
    "    df_train = transform_df(df_train)\n",
    "\n",
    "    # transform testset to right form\n",
    "    df_val = pd.DataFrame({'X': pd.Series(X_val), 'y': pd.Series(y_val)})\n",
    "    df_val = transform_df(df_val)\n",
    "\n",
    "    df_test = pd.DataFrame({'X': pd.Series(X_test), 'y': pd.Series(y_test)})\n",
    "    df_test = transform_df(df_test)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_process(df,tokenizer,max_length):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    sentences = df.X.values\n",
    "    labels = df.y.values\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,   # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                            truncation=True,\n",
    "                       )\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    #print('Check the original paragraph and converted paragrapg: ')\n",
    "    #print('Original: ', sentences[1])\n",
    "    #print('Token IDs:', input_ids[1])\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return dataset\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps):\n",
    "    print('\\nTraining and evaluating the model.')\n",
    "    \n",
    "    #store a number of quantities such as training and validation loss,validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    #store prediction and true labels\n",
    "    train_logits = []\n",
    "    train_label = []\n",
    "    best_loss = 1.0\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "    \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "    \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "    \n",
    "            model.zero_grad()        \n",
    "            loss, logits = model(b_input_ids, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=b_input_mask, \n",
    "                                 labels=b_labels)\n",
    "            \n",
    "            logits_ = logits.detach().cpu().numpy()\n",
    "            label_ids_ = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            #store prediction for the last epoch\n",
    "            if epoch_i == epochs-1:\n",
    "                train_logits.extend(logits_)\n",
    "                train_label.extend(label_ids_)\n",
    "            total_train_loss += loss.item()\n",
    "    \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "            # Update parameters and take a step using the computed gradient.Update the learning rate.\n",
    "            optimizer.step()\n",
    "        \n",
    "        #Update scheduler(lr decay) every epoch\n",
    "        lr_stat_opt = optimizer.param_groups[0][\"lr\"] #or lr_stat_scheduler = scheduler.get_last_lr()[0]\n",
    "        print('current lr is:',lr_stat_opt)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        #wandb.log({\"lr\": lr_stat_opt})\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        eval_pred = []\n",
    "        eval_label = []\n",
    "        t0 = time.time()\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "            # `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].long().to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                (loss, logits) = model(b_input_ids, \n",
    "                                       token_type_ids=None, \n",
    "                                       attention_mask=b_input_mask,\n",
    "                                       labels=b_labels)\n",
    "                \n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            #total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            eval_pred.extend(logits)\n",
    "            eval_label.extend(label_ids)\n",
    "        \n",
    "\n",
    "        eval_pred = convert_logits_tolabel(eval_pred)\n",
    "        f1_val = f1_score(eval_label,eval_pred,average='macro')\n",
    "        \n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        #avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Macro F1 score: {0:.2f}\".format(f1_val))\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "        \n",
    "        \n",
    "        #early-stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            # At this point also save a snapshot of the current model\n",
    "            torch.save(model, 'my_model_best_loss.pth')\n",
    "            \n",
    "        \n",
    "        #wandb.log({\"Training Loss\": avg_train_loss, \"Valid. Loss\":avg_val_loss,\"Valid. Macro F1\":f1_val, \"epoch\": epoch_i + 1 })\n",
    "        \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Macro F1.': f1_val,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    pd.set_option('precision', 2)\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    \n",
    "    # Plot the learning curve.\n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "    \n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    epoch_list = [i+1 for i in range(epochs)]\n",
    "    plt.xticks(epoch_list)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    train_pred = convert_logits_tolabel(train_logits)\n",
    "    \n",
    "    return train_pred,eval_pred,train_label,eval_label\n",
    "\n",
    "def get_prediction(df_test,model,batch_size,max_length,model_name):\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "\n",
    "    test_dataset = tokenize_process(df_test,tokenizer,max_length)\n",
    "    test_dataloader = DataLoader(test_dataset,sampler = SequentialSampler(test_dataset),batch_size = batch_size)\n",
    "    \n",
    "    test_logits = []\n",
    "    test_label = []\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        \"\"\"\n",
    "        Unpack this training batch from our dataloader.\n",
    "        `batch` contains three pytorch tensors: [0]: input ids; [1]: attention masks; [2]: labels \n",
    "        \"\"\"\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].long().to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            test_logits.extend(logits)\n",
    "            test_label.extend(label_ids)\n",
    "            \n",
    "    def convert_logits_tolabel(logits):\n",
    "        pred = []\n",
    "        for i in logits:\n",
    "            if i[0]> i[1]:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "        return pred\n",
    "    \n",
    "    test_logits = convert_logits_tolabel(test_logits)\n",
    "            \n",
    "    return test_logits,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_cls(df_train,df_val,\n",
    "             epochs = 10,\n",
    "             batch_size =16,\n",
    "             max_length=128,\n",
    "             model_name='roberta',\n",
    "             lr = 5e-5,\n",
    "             weight_decay = 1e-2,\n",
    "             freeze_layer_count=1,\n",
    "             scheduler_type='step',\n",
    "             decayRate=0.75):\n",
    "    \n",
    "    \n",
    "    #wandb update\n",
    "    #wandb.config.max_length = max_length\n",
    "    #wandb.config.model = model_name\n",
    "    #wandb.config.weight_decay = weight_decay\n",
    "    #wandb.config.freeze_layer_count = freeze_layer_count\n",
    "    #wandb.config.optimizer = 'AdamW'\n",
    "    #wandb.config.scheduler_type = scheduler_type\n",
    "    #wandb.config.decayRate = decayRate\n",
    "    \n",
    "    \n",
    "    print('\\n======================Doing Bert classification task======================')\n",
    "    \n",
    "    \"\"\"\n",
    "    step1: Tokenization\n",
    "    \"\"\"\n",
    "    #print('Do step1: Tokenization\\n')\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "    \n",
    "    train_dataset = tokenize_process(df_train,tokenizer,max_length)\n",
    "    val_dataset = tokenize_process(df_val,tokenizer,max_length)\n",
    "    \n",
    "    \"\"\"\n",
    "    step2: create dataloader for both training and eval set\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = batch_size\n",
    "    \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "    \n",
    "    # For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    step3: load bert model\n",
    "    \"\"\"\n",
    "    #print('Do step3: load bert model\\n')\n",
    "    model,optimizer,scheduler,total_steps = model_and_helper(epochs,train_dataloader,model_name,freeze_layer_count,lr,weight_decay,scheduler_type,decayRate)\n",
    "    \n",
    "    \"\"\"\n",
    "    step4: Training and evaluating\n",
    "    \"\"\"\n",
    "    #print('Do step4: Training and evaluating\\n')\n",
    "    train_pred,eval_pred,train_label,eval_label = Training_and_evaluating(model,train_dataloader,validation_dataloader,optimizer,epochs,scheduler,total_steps)\n",
    "    \n",
    "    \n",
    "    return train_pred,eval_pred,train_label,eval_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_and_helper(epochs,train_dataloader,\n",
    "                     model_name='roberta',\n",
    "                     freeze_layer_count=1,\n",
    "                     lr = 5e-5,\n",
    "                     weight_decay = 1e-2,\n",
    "                     scheduler_type='linear',\n",
    "                     decayRate=0.75):\n",
    "    \n",
    "    if model_name == 'bert':\n",
    "        print('\\nLoading bert model.')\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\",  num_labels = 2,  output_attentions = False, output_hidden_states = False,  return_dict = False)\n",
    "        \n",
    "        \n",
    "    elif model_name == 'roberta':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            'roberta-base', num_labels = 2, output_attentions = False, output_hidden_states = False, return_dict = False)\n",
    "        \n",
    "    else:\n",
    "        raise SystemExit('Invalid model_name, model could only be one of [bert, roberta] ')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Assign GPU if you have\n",
    "    if str(device)=='cuda':\n",
    "        model.cuda()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # We freeze here the embeddings of the model\n",
    "    if freeze_layer_count:\n",
    "        if model_name == 'bert':\n",
    "            for param in model.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.bert.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "        if model_name == 'roberta':\n",
    "            for param in model.roberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            if freeze_layer_count != -1:\n",
    "            # if freeze_layer_count == -1, we only freeze the embedding layer\n",
    "            # otherwise we freeze the first `freeze_layer_count` encoder layers\n",
    "                for layer in model.roberta.encoder.layer[:freeze_layer_count]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "        \n",
    "    \n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch), the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = lr, \n",
    "                      eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                      weight_decay = weight_decay \n",
    "                    )\n",
    "    \n",
    "    epochs = epochs\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # set learning rate decay\n",
    "    if scheduler_type=='linear': \n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value\n",
    "                                                num_training_steps = total_steps)\n",
    "    elif scheduler_type=='step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='exponential':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "        \n",
    "    elif scheduler_type=='none':\n",
    "        scheduler = None\n",
    "    \n",
    "    else:\n",
    "        raise SystemExit('Invalid scheduler_type, it could only be one of [linear, step, exponential,none] ')\n",
    "    \n",
    "\n",
    "    return model,optimizer,scheduler,total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args):\n",
    "    \n",
    "    training_pred,training_true,evaluation_pred,evaluation_true = [],[],[],[]\n",
    "    \n",
    "    training_pred.append(train_pred)\n",
    "    training_true.append(train_label)\n",
    "    evaluation_pred.append(eval_pred)\n",
    "    evaluation_true.append(eval_label)\n",
    "    \n",
    "    def flatten(t):\n",
    "        return [item for sublist in t for item in sublist]\n",
    "    \n",
    "    evaluation_true = flatten(evaluation_true)\n",
    "    evaluation_pred = flatten(evaluation_pred)\n",
    "    training_true = flatten(training_true)\n",
    "    training_pred = flatten(training_pred)\n",
    "    \n",
    "    \n",
    "    f1_train = f1_score(training_true, training_pred,average='macro')\n",
    "    f1_val = f1_score(evaluation_true, evaluation_pred,average='macro')\n",
    "    f1_test = f1_score(test_true,test_pred,average='macro')\n",
    "    \n",
    "    print({\"f1_train\": f1_train,\"f1_val\": f1_val, \"f1_test\": f1_test})\n",
    "    #wandb.log({\"f1_train\": f1_train,\"f1_val\": f1_val, \"f1_test\": f1_test})\n",
    "\n",
    "    print('\\nargs:',args)\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    #For evaluation data\n",
    "    print('classification report on test set is:\\n')\n",
    "    clas_reprt_eval = classification_report(test_true, test_pred, target_names=target_names)\n",
    "    print(clas_reprt_eval)\n",
    "    \n",
    "    print('confusion matrix on test set is:\\n')\n",
    "    cm_eval = confusion_matrix(test_true, test_pred)\n",
    "    plot_confusion_matrix(cm_eval, ['No causal relation', 'Has causal relation'], normalize=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eyeballing_prediction(X,y,args):\n",
    "    \n",
    "    # we will treat test set as our eyeballing dataset (n=100)\n",
    "    df_train, df_val, df_test = train_val_test(args['train_ratio'], args['validation_ratio'], args['test_ratio'],X,y,seed_val)\n",
    "    \n",
    "    \n",
    "    print('In this run, we have {} data points for training set.'.format(len(df_train)))\n",
    "    # over-sample the training set\n",
    "    df_train = over_sampling(df_train,seed_val)\n",
    "    \n",
    "    train_pred,eval_pred,train_label,eval_label = transformer_cls(df_train,df_val,\n",
    "                                                                          epochs = args['epochs'],\n",
    "                                                                          batch_size =args['batch_size'],\n",
    "                                                                          max_length=args['max_length'],\n",
    "                                                                          model_name=args['model_name'],\n",
    "                                                                          lr = args['lr'],\n",
    "                                                                          weight_decay = args['weight_decay'],\n",
    "                                                                          freeze_layer_count=args['freeze_layer_count'],\n",
    "                                                                          scheduler_type=args['scheduler_type'],\n",
    "                                                                          decayRate=args['decayRate'])\n",
    "    \n",
    "    #load best model\n",
    "    model = torch.load('my_model_best_loss.pth')\n",
    "    model.eval()\n",
    "    \n",
    "    test_pred,test_true = get_prediction(df_test,model,batch_size=args['batch_size'],max_length=args['max_length'],model_name=args['model_name'])\n",
    "    \n",
    "    # generate confusion matrix and classification report\n",
    "    evaluate_result(train_pred,train_label,eval_pred,eval_label,test_pred,test_true,args)\n",
    "\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #clear output if you want\n",
    "    #clear_output(wait=True)\n",
    "    return test_pred,test_true,df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_args = {\n",
    "    'epochs': 14,\n",
    "    'batch_size': 20,\n",
    "    'train_ratio': 0.75,\n",
    "    'validation_ratio': 0.137,\n",
    "    'test_ratio': 0.113, #generate 100 data points\n",
    "    'max_length': 128,\n",
    "    'model_name': 'roberta',\n",
    "    'lr': 1e-05,\n",
    "    'weight_decay': 0.05,\n",
    "     'freeze_layer_count': 6,\n",
    "    'scheduler_type': 'step',\n",
    "    'decayRate': 0.75\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X training set shape, X validation set shape, X test set shape]: (658,) (120,) (100,)\n",
      "In this run, we have 658 data points for training set.\n",
      "\n",
      "We totally have 956 training data after oversampling.\n",
      "\n",
      "======================Doing Bert classification task======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating the model.\n",
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 1e-05\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.53\n",
      "  Validation Loss: 0.68\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 7.500000000000001e-06\n",
      "\n",
      "  Average training loss: 0.68\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.70\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 5.625000000000001e-06\n",
      "\n",
      "  Average training loss: 0.59\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.68\n",
      "  Validation Loss: 0.57\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 4.218750000000001e-06\n",
      "\n",
      "  Average training loss: 0.50\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.67\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 3.1640625000000007e-06\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.65\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 2.3730468750000007e-06\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.68\n",
      "  Validation Loss: 0.58\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 1.7797851562500005e-06\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.66\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 1.3348388671875005e-06\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.69\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 1.0011291503906254e-06\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.68\n",
      "  Validation Loss: 0.63\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 7.508468627929691e-07\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.69\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 5.631351470947268e-07\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.69\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 4.223513603210451e-07\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.69\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 3.1676352024078383e-07\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:00:08\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.69\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "  Batch    40  of     48.    Elapsed: 0:00:06.\n",
      "current lr is: 2.3757264018058787e-07\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:00:07\n",
      "\n",
      "Running Validation...\n",
      "  Macro F1 score: 0.69\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:50 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_train': 0.9142019087645565, 'f1_val': 0.6873982416151091, 'f1_test': 0.5810244627652386}\n",
      "\n",
      "args: {'epochs': 14, 'batch_size': 20, 'train_ratio': 0.75, 'validation_ratio': 0.137, 'test_ratio': 0.113, 'max_length': 128, 'model_name': 'roberta', 'lr': 1e-05, 'weight_decay': 0.05, 'freeze_layer_count': 6, 'scheduler_type': 'step', 'decayRate': 0.75}\n",
      "classification report on test set is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.31      0.45      0.37        20\n",
      "     class 1       0.85      0.75      0.79        80\n",
      "\n",
      "    accuracy                           0.69       100\n",
      "   macro avg       0.58      0.60      0.58       100\n",
      "weighted avg       0.74      0.69      0.71       100\n",
      "\n",
      "confusion matrix on test set is:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pred,test_true,df_test = eyeballing_prediction(X,y,best_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prediction to df_test, then save it as csv file\n",
    "\n",
    "df_test['prediction'] = test_pred\n",
    "df_test.columns = ['text', 'label', 'prediction']\n",
    "\n",
    "# To keep consistency, we will use df to relocate those data points in df_test,\n",
    "# because some data points in df_test might be truncated \n",
    "df = pd.DataFrame({'X': pd.Series(X), 'y': pd.Series(y)})\n",
    "eyeballing = df.loc[list(df_test.index)]\n",
    "\n",
    "eyeballing['prediction'] = test_pred\n",
    "eyeballing.columns = ['text', 'label', 'prediction']\n",
    "\n",
    "eyeballing.loc[eyeballing['label'] == True, 'label'] = 1\n",
    "eyeballing.loc[eyeballing['label'] == False, 'label'] = 0\n",
    "eyeballing['label'] = eyeballing['label'].astype(int)\n",
    "\n",
    "# add Speech_ID\n",
    "Speech_ID = []\n",
    "paragraph = []\n",
    "for i in list(eyeballing.index):\n",
    "    Speech_ID.append(i.split(' ')[0])\n",
    "    paragraph.append(i.split(' ')[1])\n",
    "    \n",
    "eyeballing['source'] = Speech_ID\n",
    "eyeballing['paragraph'] = paragraph\n",
    "eyeballing['Speech_ID'] = None\n",
    "\n",
    "df_source = speeches[speeches['Speech_ID'].isin(sources)]\n",
    "\n",
    "for i in df_source['Speech_ID'].values:\n",
    "    eyeballing.loc[eyeballing[eyeballing['source']==str(i)].index, \"Speech_ID\"] =df_source[df_source['Speech_ID'] == i]['Speech_Identifier'].values[0]\n",
    "\n",
    "eyeballing = eyeballing[['Speech_ID', 'text', 'label', 'prediction', 'source', 'paragraph']]\n",
    "eyeballing = eyeballing.drop(columns=['source', 'paragraph'])\n",
    "eyeballing.to_excel(\"Eyeballing-predictions.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
