{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f2bc42-9f46-4a4b-a491-5422e181324a",
   "metadata": {},
   "source": [
    "# Doccano data\n",
    "\n",
    "Export data to doccano (http://doccano.herokuapp.com/) format: json: { \"id\": 0, \"data\": \"text\", \"label\": [] }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365667e3-75a5-46ad-b068-8f161013a89c",
   "metadata": {},
   "source": [
    "## 1. Convert data from tables and text to JSONL for Doccano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd3a8f4-36ce-4928-844e-b2ba6a88b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from termcolor import colored\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f12d2e-a581-41c4-ae5a-7fc268260230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_file(file_name):\n",
    "    return pd.read_csv(file_name, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3092ff-50e9-48e2-b9b9-91f6592c7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_id(file_name, speeches):\n",
    "    try:\n",
    "        file_name_parts = file_name.split()\n",
    "        date = file_name_parts[0]\n",
    "        speaker = list(file_name_parts[1].split(\"_\")[0])\n",
    "        speaker[0] = speaker[0].upper()\n",
    "        speaker = \"\".join(speaker)\n",
    "        speech_identifier = f\"{speaker} {date}\"\n",
    "        speech_identifier = fix_speech_identifier(speech_identifier)\n",
    "        return int(speeches[speeches[\"Speech_Identifier\"] == speech_identifier][\"Speech_ID\"])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e517658-e53c-4c10-8121-bf764a377e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_ids(speech_id, speech_contents):\n",
    "    paragraph_ids = {}\n",
    "    try:\n",
    "        for i, row in speech_contents[speech_contents[\"Speech_ID\"] == speech_id].iterrows():\n",
    "            paragraph_ids[row[\"Speech_Content_ID\"]] = row[\"Speech_Content_Title\"]\n",
    "    except:\n",
    "        pass\n",
    "    return paragraph_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefb8807-cafc-4ed5-935f-51c77345088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_paragraphs(speech_id, paragraph_ids, map_contents, file_name):\n",
    "    paragraph_values = {}\n",
    "    for i, row in map_contents[map_contents[\"Content_Speech_ID\"] == speech_id].iterrows():\n",
    "        if row[\"Content_Source_ID\"] not in paragraph_ids:\n",
    "            print(colored(f'warning: unknown paragraph id {row[\"Content_Source_ID\"]} for document {speech_id}; file name: {file_name}', \"red\"))\n",
    "        else:\n",
    "            paragraph_values[f'{speech_id} {paragraph_ids[row[\"Content_Source_ID\"]]}'] = True\n",
    "    return paragraph_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7c2978-0ba5-49df-b0ff-dfecf7a63e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paragraphs(file_name):\n",
    "    paragraph_list = []\n",
    "    data_file = open(file_name, \"r\", encoding=\"latin1\")\n",
    "    for line in data_file:\n",
    "        paragraph_list.append(line.strip())\n",
    "    data_file.close()\n",
    "    return paragraph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182afb0e-9b09-406b-816c-22ce9d800188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_paragraphs(paragraph_list, paragraph_values, speech_id):\n",
    "    paragraph_texts = {}\n",
    "    use_paragraph = False\n",
    "    for paragraph in paragraph_list:\n",
    "        tokens = paragraph.strip().split()\n",
    "        if len(tokens) > 0 and re.search(r'^\\d+-\\d+:*$', tokens[0]):\n",
    "            use_paragraph = True\n",
    "            key = re.sub(\":\", \"\", tokens[0])\n",
    "            key = f\"{speech_id} {key}\"\n",
    "            tokens.pop(0)\n",
    "            if len(tokens) > 0 and tokens[0] == \":\":\n",
    "                tokens.pop(0)\n",
    "        if len(tokens) > 0 and use_paragraph:\n",
    "            paragraph_texts[key] = \" \".join(tokens)\n",
    "            if key not in paragraph_values:\n",
    "                paragraph_values[key] = False\n",
    "            use_paragraph = False\n",
    "    return paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84278bbc-0faa-4be9-be60-483dfc240761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_language(paragraph_texts):\n",
    "    text = \" \".join(paragraph_texts.values())\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e563be5f-e953-4ebf-a2a0-66f62ed45913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_speech_identifier(speech_identifier):\n",
    "    speech_identifier = re.sub(\"Simor 2010-05-25\", \"Simor 2010-05-26\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"^Mario \", \"Draghi \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"^PM \", \"Cameron \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"^Thorning \", \"Thorning-Schmidt \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Remarks 2009-12-11\", \"Honohan 2009-12-11\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"This 2013-02-11\", \"Cameron 2013-02-11\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Mervyn \", \"King \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Patrick 2013-03-19\", \"Honohan 2013-03-19\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Statement 2014-12-18\", \"Kenny 2014-12-19\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Speech 2012-06-29\", \"Cameron 2012-06-29\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"The 2012-01-30\", \"Cameron 2012-01-30\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Orban \", \"Orbán \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"The 2014-10-24\", \"Cameron 2014-10-24\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Speech 2013-03-07\", \"Kenny 2013-07-03\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"David 2014-11-10\", \"Cameron 2014-11-10\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Statement 2012-07-04\", \"Kenny 2012-07-04\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Schröder\", \"Schroeder\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Schroeder 1998-12-14\", \"Schroeder 1999-12-14\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Schroeder 2001-10-26\", \"Schroeder 2001-10-16\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Hollande 2015-05-19\", \"Hollande 2015-03-19\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Fernandez 2009-11-23\", \"Fernández Ordóñez  2009-11-23\", speech_identifier)\n",
    "    return speech_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578bf675-7db0-4e2a-9d3d-958a9e907b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase(text, phrase, start_index=0):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        phrase = phrase.lower()\n",
    "        for i in range(start_index, len(text)-len(phrase)):\n",
    "            if text[i: i+len(phrase)] == phrase:\n",
    "                return i\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93a016eb-c878-4d6f-adcf-3525120437fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker(file_name):\n",
    "    file_name_parts = re.split(\" |_\", file_name)\n",
    "    return file_name_parts[1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cff1b12-3353-4f42-bb0e-17147b9e5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doccano_data(speech_id, paragraph_texts, paragraph_ids, map_contents, misses, speaker):\n",
    "    annotations = []\n",
    "    for paragraph_id in paragraph_ids:\n",
    "        paragraph_text_id = f\"{speech_id} {paragraph_ids[paragraph_id]}\"\n",
    "        if paragraph_text_id in paragraph_texts:\n",
    "            paragraph_text = paragraph_texts[paragraph_text_id]\n",
    "            for i, row in map_contents[map_contents[\"Content_Source_ID\"] == paragraph_id].iterrows():\n",
    "                concept_1_start = find_phrase(paragraph_text, row[\"Content_Concept_1\"])\n",
    "                concept_2_start = find_phrase(paragraph_text, row[\"Content_Concept_2\"])\n",
    "                explanation_start = find_phrase(paragraph_text, row[\"Content_Relation_Explanation\"])\n",
    "                if (not pd.isna(row[\"Content_Concept_1\"]) and \n",
    "                    explanation_start != None and \n",
    "                    not pd.isna(row[\"Content_Concept_2\"])):\n",
    "                    annotation = { \"text\": paragraph_text, \n",
    "                                   \"label\": [ [ explanation_start, explanation_start + len(row[\"Content_Relation_Explanation\"]), \"Content_Relation_Explanation\" ] ],\n",
    "                                   \"source_id\": paragraph_id,\n",
    "                                   \"speech_id\": speech_id,\n",
    "                                   \"paragraph_id\": paragraph_ids[paragraph_id] }\n",
    "                    if concept_1_start != None:\n",
    "                        annotation[\"label\"] = [ [ concept_1_start, concept_1_start + len(row[\"Content_Concept_1\"]), \"Content_Concept_1\" ] ] + annotation[\"label\"]\n",
    "                    else:\n",
    "                        annotation[\"missing concept 1\"] = row[\"Content_Concept_1\"]\n",
    "                    if concept_2_start != None:\n",
    "                        annotation[\"label\"].append([ concept_2_start, concept_2_start + len(row[\"Content_Concept_2\"]), \"Content_Concept_2\" ])\n",
    "                    else:\n",
    "                        annotation[\"missing concept 2\"] = row[\"Content_Concept_2\"]\n",
    "                    annotation[\"merged concept 1\"] = row[\"Content_Merged_Concept_1\"]\n",
    "                    annotation[\"merged concept 2\"] = row[\"Content_Merged_Concept_2\"]\n",
    "                    annotation[\"speaker\"] = speaker\n",
    "                    annotations.append(annotation)\n",
    "                \n",
    "                if concept_1_start != None and explanation_start != None and concept_2_start != None:\n",
    "                    misses[\"all\"] += 1\n",
    "                elif concept_1_start != None and concept_2_start != None:\n",
    "                    misses[\"both\"] += 1\n",
    "                elif explanation_start == None:\n",
    "                    if pd.isna(row[\"Content_Relation_Explanation\"]):\n",
    "                        misses[\"Content_Relation_Explanation NaN\"] += 1\n",
    "                    else:\n",
    "                        misses[\"Content_Relation_Explanation\"] += 1\n",
    "                elif concept_1_start == None:\n",
    "                    misses[\"Content_Concept_1\"] += 1\n",
    "                elif concept_2_start == None:\n",
    "                    misses[\"Content_Concept_2\"] += 1\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74bf461-67f2-4960-a4da-4cd7176e141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(speeches, speech_contents, map_contents, text_directory):\n",
    "    files = os.listdir(text_directory)\n",
    "    nbr_of_files = 0\n",
    "    nbr_of_skipped = 0\n",
    "    annotations = []\n",
    "    misses = { \"Content_Concept_1\": 0, \n",
    "               \"Content_Concept_2\": 0, \n",
    "               \"Content_Relation_Explanation\":0, \n",
    "               \"Content_Relation_Explanation NaN\": 0, \n",
    "               \"all\": 0, \n",
    "               \"both\": 0 }\n",
    "    for file_name in files:\n",
    "        speech_id = get_speech_id(file_name, speeches)\n",
    "        if speech_id == None:\n",
    "            print(f\"skipping file {file_name}\")\n",
    "            nbr_of_skipped += 1\n",
    "        else:\n",
    "            paragraph_ids = get_paragraph_ids(speech_id, speech_contents)\n",
    "            paragraph_values = check_paragraphs(speech_id, paragraph_ids, map_contents, file_name)\n",
    "            paragraph_list = read_paragraphs(f\"{text_directory}/{file_name}\")\n",
    "            paragraph_texts = select_paragraphs(paragraph_list, paragraph_values, speech_id)\n",
    "            speaker = get_speaker(file_name)\n",
    "            language = guess_language(paragraph_texts)\n",
    "            if language == \"en\":\n",
    "                annotations.extend(make_doccano_data(speech_id, paragraph_texts, paragraph_ids, map_contents, misses, speaker))\n",
    "                if len(paragraph_texts) != len(paragraph_values):\n",
    "                    print(colored(f\"warning: mismatch meta data ({len(paragraph_values)}) vs\", \"red\"), end=\" \")\n",
    "                    print(colored(f\"file ({len(paragraph_texts)}) for file {file_name}\", \"red\"))\n",
    "                nbr_of_files += 1\n",
    "            else:\n",
    "                print(f\"skipping file in language {language}: {file_name}\")\n",
    "                nbr_of_skipped += 1\n",
    "    print(f\"read {nbr_of_files} files; skipped {nbr_of_skipped} file\", end=\"\")\n",
    "    if nbr_of_skipped != 1:\n",
    "        print(\"s\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "    print(misses)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed6d3ba-c138-429a-9f70-01d9b61159b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_annotations(annotations, file_name):\n",
    "    out_file = open(file_name, \"w\")\n",
    "    for annotation in annotations:\n",
    "        print(json.dumps(annotation), file=out_file)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d48cb349-f238-4ef1-a4ec-801da9f7ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/annotations/\"\n",
    "TEXT_DIR = \"../data/documents/\"\n",
    "MAP_CONTENTS = DATA_DIR + \"Map_Contents-20200726.csv\"\n",
    "SPEECH_CONTENTS_FILE = DATA_DIR + \"Speech_Contents-20210520.txt\"\n",
    "SPEECHES_FILE = DATA_DIR + \"Speeches-20210520.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3293cf9-e082-4226-9295-0b0d9225ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_contents = read_data_file(MAP_CONTENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d67bf158-b81c-48b3-9d16-cc04f88c3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_contents = read_data_file(SPEECH_CONTENTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56f3cd5-93ac-44e0-9bd7-87d2b13781e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = read_data_file(SPEECHES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ff840f7-0f07-4e92-aa63-e29ad36ff7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mwarning: unknown paragraph id nan for document 208; file name: 2012-02-24 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2015-01-19 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language de: 2013-11-21 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language fr: 2013-04-17 Hollande SFM2020 ann fr.txt\n",
      "skipping file in language fr: 2009-12-01 Sarkozy Elysee (Economy) ann fr.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 335; file name: 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs\u001b[0m \u001b[31mfile (20) for file 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs\u001b[0m \u001b[31mfile (20) for file 2014-07-18 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language nl: 2011-10-28 Knot dnb_01 ANN NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (11) vs\u001b[0m \u001b[31mfile (10) for file 2012-07-17 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "skipping file placeholder.txt\n",
      "\u001b[31mwarning: mismatch meta data (2) vs\u001b[0m \u001b[31mfile (1) for file 2010-03-29 Brown sfm2020 ann.txt\u001b[0m\n",
      "skipping file in language fr: 2009-12-14 Sarkozy Elysee (Economy) ann fr.txt\n",
      "skipping file in language fr: 2010-04-20 Barroso European Commission ann fr.txt\n",
      "skipping file in language nl: 2011-09-27 Rutte Rijksoverheid ann.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs\u001b[0m \u001b[31mfile (13) for file 2013-05-31 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (22) vs\u001b[0m \u001b[31mfile (19) for file 2013-10-08 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-02-19 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs\u001b[0m \u001b[31mfile (12) for file 2011-05-20 Weidmann Deutsche Bundesbank_01 ANN.txt\u001b[0m\n",
      "skipping file in language fr: 2012-08-30 Hollande SFM2020 ann fr.txt\n",
      "skipping file in language de: 2014-02-27 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language fr: 2011-01-13 Sarkozy gb ann.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs\u001b[0m \u001b[31mfile (3) for file 2011-11-10 Orban London ann.txt\u001b[0m\n",
      "skipping file in language nl: 2011-04-06 Rutte FD evenement ann NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (19) vs\u001b[0m \u001b[31mfile (18) for file 2013-02-26 Rohde Danmarks Nationalbank ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (10) vs\u001b[0m \u001b[31mfile (7) for file 1998-12-14 Schroeder ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (7) vs\u001b[0m \u001b[31mfile (6) for file 2012-12-07 Simor Bis.org ann.txt\u001b[0m\n",
      "skipping file in language de: 2012-01-06 Rutte CSU klausurtagung ann G.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs\u001b[0m \u001b[31mfile (3) for file 2011-11-12 Rutte Trilateral Commission - code ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (24) vs\u001b[0m \u001b[31mfile (22) for file 2012-07-11 Rajoy La Moncloa ann.txt\u001b[0m\n",
      "skipping file in language unk: 2012-07-26 Barroso European Commission.txt\n",
      "read 118 files; skipped 16 files\n",
      "{'Content_Concept_1': 917, 'Content_Concept_2': 346, 'Content_Relation_Explanation': 606, 'Content_Relation_Explanation NaN': 700, 'all': 631, 'both': 417}\n"
     ]
    }
   ],
   "source": [
    "annotations = read_data(speeches, speech_contents, map_contents, text_directory=TEXT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16c025-a575-40ea-8fda-ef6812978f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_annotations(annotations, \"annotations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd5522-d5c2-47bd-b163-5647a0a7fb0a",
   "metadata": {},
   "source": [
    "## 2. Add annotation corrections of the summer of 2021\n",
    "\n",
    "Challenge: during the correction process (?) 27 records were removed. The missing records can be identified by the values in the field \"id\" of the corrected/updated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e98e5a-336a-4620-a7ec-fcaada49dc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Today I want to send a clear message to the people of this great country, of Greece. I know that many people feel without hope. Many are making extremely difficult sacrifices. And many people ask why they should do more. I understand those concerns. And I agree that some of the efforts seem unfair. But I ask people to recognise the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society. So this is why it is the right approach to ask Greece to reform, to increase its competitiveness to have a viable future, irrespective of the crisis. You, in Greece, with our support, need to rebuild your country, your structures, your administration, your economy to increase the competitiveness of Greece. And the best hope of a return to growth and job creation is inside the euro area. Staying in the euro is the best chance to avoid worse hardship and difficulties to the Greek people, namely for those in a more vulnerable position',\n",
       " 'label': [[0, 2, 'Content_Relation_Explanation'],\n",
       "  [734, 772, 'Content_Concept_2']],\n",
       " 'source_id': 3487,\n",
       " 'speech_id': 536,\n",
       " 'paragraph_id': '1-2',\n",
       " 'missing concept 1': 'you in Greece, with out support, you need to rebuild your country',\n",
       " 'merged concept 1': 'taking your own responsibility',\n",
       " 'merged concept 2': 'competitiveness',\n",
       " 'speaker': 'barroso'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "959fbd15-de0d-471d-9a83-8bc20b0e90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_updated_annotations(file_name):\n",
    "    infile = open(file_name, \"r\")\n",
    "    updated_annotations = []\n",
    "    paragraphs = {}\n",
    "    for text_line in infile:\n",
    "        json_line = json.loads(text_line)\n",
    "        paragraph_key = \" \".join([str(json_line[\"source_id\"]),\n",
    "                                  str(json_line[\"speech_id\"]),\n",
    "                                  json_line[\"paragraph_id\"]])\n",
    "        if paragraph_key in paragraphs:\n",
    "            paragraphs[paragraph_key].append(len(updated_annotations))\n",
    "        else:\n",
    "            paragraphs[paragraph_key] = [len(updated_annotations)]\n",
    "        updated_annotations.append(json_line)\n",
    "    infile.close()\n",
    "    return [ updated_annotations, paragraphs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69e84318-ac41-4fa4-afbe-9f0142fda290",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_annotations, paragraphs = read_updated_annotations(\"../../data/femke-20211012.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54a241f6-d5b3-4f4f-8d1a-810b6565834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 6725 2\n",
      "82 6745 1\n",
      "140 6805 2\n",
      "267 6933 1\n",
      "358 7030 6\n",
      "791 7464 1\n",
      "1379 8054 2\n",
      "1502 8181 4\n",
      "1565 8251 7\n",
      "1757 8444 1\n",
      "skipped: 27\n"
     ]
    }
   ],
   "source": [
    "last_id = 6659\n",
    "skipped = 0\n",
    "for i in range(0, len(updated_annotations)):\n",
    "    if updated_annotations[i][\"id\"] != last_id + 1:\n",
    "        print(i, updated_annotations[i][\"id\"], updated_annotations[i][\"id\"] - last_id - 1)\n",
    "        skipped += updated_annotations[i][\"id\"] - last_id - 1\n",
    "    last_id = updated_annotations[i][\"id\"]\n",
    "\n",
    "print(f\"skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90a4076e-13bb-4f85-9aa9-1b09c1e90b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1894 1867 27\n"
     ]
    }
   ],
   "source": [
    "print(len(annotations), len(updated_annotations), len(annotations) - len(updated_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5ac26b9-00a0-4ff7-b533-e4e38c7f3711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6660,\n",
       " 'data': 'Today I want to send a clear message to the people of this great country, of Greece. I know that many people feel without hope. Many are making extremely difficult sacrifices. And many people ask why they should do more. I understand those concerns. And I agree that some of the efforts seem unfair. But I ask people to recognise the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society. So this is why it is the right approach to ask Greece to reform, to increase its competitiveness to have a viable future, irrespective of the crisis. You, in Greece, with our support, need to rebuild your country, your structures, your administration, your economy to increase the competitiveness of Greece. And the best hope of a return to growth and job creation is inside the euro area. Staying in the euro is the best chance to avoid worse hardship and difficulties to the Greek people, namely for those in a more vulnerable position',\n",
       " 'label': [[734, 772, 'Content_Concept_2'],\n",
       "  [731, 733, 'Content_Relation_Explanation'],\n",
       "  [616, 678, 'Content_Concept_1']],\n",
       " 'source_id': 3487,\n",
       " 'speech_id': 536,\n",
       " 'paragraph_id': '1-2',\n",
       " 'missing concept 1': 'you in Greece, with out support, you need to rebuild your country'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06ede156-0ea3-4ab2-a8ea-d89a80fd55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_annotations(annotation, updated_annotation):\n",
    "    if annotation[\"text\"] != updated_annotation[\"data\"]:\n",
    "        print(\"text mismatch\")\n",
    "    if annotation[\"source_id\"] != updated_annotation[\"source_id\"]:\n",
    "        print(\"source_id mismatch\")\n",
    "    if annotation[\"speech_id\"] != updated_annotation[\"speech_id\"]:\n",
    "        print(\"speech_id mismatch\")\n",
    "    if annotation[\"paragraph_id\"] != updated_annotation[\"paragraph_id\"]:\n",
    "        print(\"paragraph_id mismatch\")\n",
    "    if \"missing concept 1\" in annotation and annotation[\"missing concept 1\"] != updated_annotation[\"missing concept 1\"]:\n",
    "        print(\"missing concept 1 mismatch\")\n",
    "    if \"missing concept 2\" in annotation and annotation[\"missing concept 2\"] != updated_annotation[\"missing concept 2\"]:\n",
    "        print(\"missing concept 2 mismatch\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0cb1cab-e9bc-4ad0-99c1-fd96a01f8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_updated_labels(annotations, updated_annotations):\n",
    "    annotations_id = 0\n",
    "    updated_annotations_id = 0\n",
    "    expected_id = 6660\n",
    "    while annotations_id < len(annotations):\n",
    "        if updated_annotations[updated_annotations_id][\"id\"] != expected_id:\n",
    "            for i in range(0, updated_annotations[updated_annotations_id][\"id\"] - expected_id):\n",
    "                annotations[annotations_id + i][\"note\"] = \"deleted during annotation\"\n",
    "                annotations[annotations_id + i][\"id\"] = annotations_id + i + 1\n",
    "            annotations_id += updated_annotations[updated_annotations_id][\"id\"] - expected_id\n",
    "        compare_annotations(annotations[annotations_id], updated_annotations[updated_annotations_id])\n",
    "        annotations[annotations_id][\"id\"] = annotations_id + 1\n",
    "        annotations[annotations_id][\"label\"] = copy.deepcopy(updated_annotations[updated_annotations_id][\"label\"])\n",
    "        expected_id = updated_annotations[updated_annotations_id][\"id\"] + 1\n",
    "        updated_annotations_id += 1\n",
    "        annotations_id += 1\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55edfbf2-9035-425c-a87d-f36606f8ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_annotations = add_updated_labels(copy.deepcopy(annotations), updated_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93dce2fa-5997-409e-af56-742f62bcca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_annotations(merged_annotations, \"annotations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87c9f0-4778-4820-8607-548943dce478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
