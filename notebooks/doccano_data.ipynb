{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f2bc42-9f46-4a4b-a491-5422e181324a",
   "metadata": {},
   "source": [
    "# Doccano data\n",
    "\n",
    "Export data to doccano (http://doccano.herokuapp.com/) format: json: { \"id\": 0, \"data\": \"text\", \"label\": [] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1dd3a8f4-36ce-4928-844e-b2ba6a88b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from termcolor import colored\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f12d2e-a581-41c4-ae5a-7fc268260230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_file(file_name):\n",
    "    return pd.read_csv(file_name, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3092ff-50e9-48e2-b9b9-91f6592c7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_id(file_name, speeches):\n",
    "    try:\n",
    "        file_name_parts = file_name.split()\n",
    "        date = file_name_parts[0]\n",
    "        speaker = list(file_name_parts[1].split(\"_\")[0])\n",
    "        speaker[0] = speaker[0].upper()\n",
    "        speaker = \"\".join(speaker)\n",
    "        speech_identifier = f\"{speaker} {date}\"\n",
    "        speech_identifier = fix_speech_identifier(speech_identifier)\n",
    "        return int(speeches[speeches[\"Speech_Identifier\"] == speech_identifier][\"Speech_ID\"])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e517658-e53c-4c10-8121-bf764a377e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_ids(speech_id, speech_contents):\n",
    "    paragraph_ids = {}\n",
    "    try:\n",
    "        for i, row in speech_contents[speech_contents[\"Speech_ID\"] == speech_id].iterrows():\n",
    "            paragraph_ids[row[\"Speech_Content_ID\"]] = row[\"Speech_Content_Title\"]\n",
    "    except:\n",
    "        pass\n",
    "    return paragraph_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cefb8807-cafc-4ed5-935f-51c77345088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_paragraphs(speech_id, paragraph_ids, map_contents, file_name):\n",
    "    paragraph_values = {}\n",
    "    for i, row in map_contents[map_contents[\"Content_Speech_ID\"] == speech_id].iterrows():\n",
    "        if row[\"Content_Source_ID\"] not in paragraph_ids:\n",
    "            print(colored(f'warning: unknown paragraph id {row[\"Content_Source_ID\"]} for document {speech_id}; file name: {file_name}', \"red\"))\n",
    "        else:\n",
    "            paragraph_values[f'{speech_id} {paragraph_ids[row[\"Content_Source_ID\"]]}'] = True\n",
    "    return paragraph_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b7c2978-0ba5-49df-b0ff-dfecf7a63e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paragraphs(file_name):\n",
    "    paragraph_list = []\n",
    "    data_file = open(file_name, \"r\", encoding=\"latin1\")\n",
    "    for line in data_file:\n",
    "        paragraph_list.append(line.strip())\n",
    "    data_file.close()\n",
    "    return paragraph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "182afb0e-9b09-406b-816c-22ce9d800188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_paragraphs(paragraph_list, paragraph_values, speech_id):\n",
    "    paragraph_texts = {}\n",
    "    use_paragraph = False\n",
    "    for paragraph in paragraph_list:\n",
    "        tokens = paragraph.strip().split()\n",
    "        if len(tokens) > 0 and re.search(r'^\\d+-\\d+:*$', tokens[0]):\n",
    "            use_paragraph = True\n",
    "            key = re.sub(\":\", \"\", tokens[0])\n",
    "            key = f\"{speech_id} {key}\"\n",
    "            tokens.pop(0)\n",
    "            if len(tokens) > 0 and tokens[0] == \":\":\n",
    "                tokens.pop(0)\n",
    "        if len(tokens) > 0 and use_paragraph:\n",
    "            paragraph_texts[key] = \" \".join(tokens)\n",
    "            if key not in paragraph_values:\n",
    "                paragraph_values[key] = False\n",
    "            use_paragraph = False\n",
    "    return paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84278bbc-0faa-4be9-be60-483dfc240761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_language(paragraph_texts):\n",
    "    text = \" \".join(paragraph_texts.values())\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e563be5f-e953-4ebf-a2a0-66f62ed45913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_speech_identifier(speech_identifier):\n",
    "    speech_identifier = re.sub(\"Simor 2010-05-25\", \"Simor 2010-05-26\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"^Mario \", \"Draghi \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"^PM \", \"Cameron \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"^Thorning \", \"Thorning-Schmidt \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Remarks 2009-12-11\", \"Honohan 2009-12-11\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"This 2013-02-11\", \"Cameron 2013-02-11\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Mervyn \", \"King \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Patrick 2013-03-19\", \"Honohan 2013-03-19\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Statement 2014-12-18\", \"Kenny 2014-12-19\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Speech 2012-06-29\", \"Cameron 2012-06-29\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"The 2012-01-30\", \"Cameron 2012-01-30\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Orban \", \"Orbán \", speech_identifier)\n",
    "    speech_identifier = re.sub(\"The 2014-10-24\", \"Cameron 2014-10-24\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Speech 2013-03-07\", \"Kenny 2013-07-03\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"David 2014-11-10\", \"Cameron 2014-11-10\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Statement 2012-07-04\", \"Kenny 2012-07-04\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Schröder\", \"Schroeder\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Schroeder 1998-12-14\", \"Schroeder 1999-12-14\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Schroeder 2001-10-26\", \"Schroeder 2001-10-16\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Hollande 2015-05-19\", \"Hollande 2015-03-19\", speech_identifier)\n",
    "    speech_identifier = re.sub(\"Fernandez 2009-11-23\", \"Fernández Ordóñez  2009-11-23\", speech_identifier)\n",
    "    return speech_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "578bf675-7db0-4e2a-9d3d-958a9e907b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase(text, phrase, start_index=0):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        phrase = phrase.lower()\n",
    "        for i in range(start_index, len(text)-len(phrase)):\n",
    "            if text[i: i+len(phrase)] == phrase:\n",
    "                return i\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8cff1b12-3353-4f42-bb0e-17147b9e5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doccano_data(speech_id, paragraph_texts, paragraph_ids, map_contents, misses):\n",
    "    annotations = []\n",
    "    for paragraph_id in paragraph_ids:\n",
    "        paragraph_text_id = f\"{speech_id} {paragraph_ids[paragraph_id]}\"\n",
    "        if paragraph_text_id in paragraph_texts:\n",
    "            paragraph_text = paragraph_texts[paragraph_text_id]\n",
    "            for i, row in map_contents[map_contents[\"Content_Source_ID\"] == paragraph_id].iterrows():\n",
    "                concept_1_start = find_phrase(paragraph_text, row[\"Content_Concept_1\"])\n",
    "                concept_2_start = find_phrase(paragraph_text, row[\"Content_Concept_2\"])\n",
    "                explanation_start = find_phrase(paragraph_text, row[\"Content_Relation_Explanation\"])\n",
    "                if not pd.isna(row[\"Content_Concept_1\"]) and explanation_start != None and not pd.isna(row[\"Content_Concept_2\"]):\n",
    "                    annotation = { \"text\": paragraph_text, \n",
    "                                   \"label\": [ [ explanation_start, explanation_start + len(row[\"Content_Relation_Explanation\"]), \"Content_Relation_Explanation\" ] ],\n",
    "                                   \"source_id\": paragraph_id,\n",
    "                                   \"speech_id\": speech_id,\n",
    "                                   \"paragraph_id\": paragraph_ids[paragraph_id] }\n",
    "                    if concept_1_start != None:\n",
    "                        annotation[\"label\"] = [ [ concept_1_start, concept_1_start + len(row[\"Content_Concept_1\"]), \"Content_Concept_1\" ] ] + annotation[\"label\"]\n",
    "                    else:\n",
    "                        annotation[\"missing concept 1\"] = row[\"Content_Concept_1\"]\n",
    "                    if concept_2_start != None:\n",
    "                        annotation[\"label\"].append([ concept_2_start, concept_2_start + len(row[\"Content_Concept_2\"]), \"Content_Concept_2\" ])\n",
    "                    else:\n",
    "                        annotation[\"missing concept 2\"] = row[\"Content_Concept_2\"]\n",
    "                    annotations.append(annotation)\n",
    "                \n",
    "                if concept_1_start != None and explanation_start != None and concept_2_start != None:\n",
    "                    misses[\"all\"] += 1\n",
    "                elif concept_1_start != None and concept_2_start != None:\n",
    "                    misses[\"both\"] += 1\n",
    "                elif explanation_start == None:\n",
    "                    if pd.isna(row[\"Content_Relation_Explanation\"]):\n",
    "                        misses[\"Content_Relation_Explanation NaN\"] += 1\n",
    "                    else:\n",
    "                        misses[\"Content_Relation_Explanation\"] += 1\n",
    "                elif concept_1_start == None:\n",
    "                    misses[\"Content_Concept_1\"] += 1\n",
    "                    #print(\"NO ###\", row[\"Content_Concept_1\"], \"### in ###\", paragraph_text)\n",
    "                elif concept_2_start == None:\n",
    "                    misses[\"Content_Concept_2\"] += 1\n",
    "                    #print(\"NO ###\", row[\"Content_Concept_2\"], \"### in ###\", paragraph_text)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f74bf461-67f2-4960-a4da-4cd7176e141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(speeches, speech_contents, map_contents, text_directory):\n",
    "    files = os.listdir(text_directory)\n",
    "    nbr_of_files = 0\n",
    "    nbr_of_skipped = 0\n",
    "    annotations = []\n",
    "    misses = { \"Content_Concept_1\": 0, \"Content_Concept_2\": 0, \"Content_Relation_Explanation\":0, \"Content_Relation_Explanation NaN\": 0, \"all\": 0, \"both\": 0 }\n",
    "    for file_name in files:\n",
    "        speech_id = get_speech_id(file_name, speeches)\n",
    "        if speech_id == None:\n",
    "            print(f\"skipping file {file_name}\")\n",
    "            nbr_of_skipped += 1\n",
    "        else:\n",
    "            paragraph_ids = get_paragraph_ids(speech_id, speech_contents)\n",
    "            paragraph_values = check_paragraphs(speech_id, paragraph_ids, map_contents, file_name)\n",
    "            paragraph_list = read_paragraphs(f\"txt/{file_name}\")\n",
    "            paragraph_texts = select_paragraphs(paragraph_list, paragraph_values, speech_id)\n",
    "            language = guess_language(paragraph_texts)\n",
    "            if language == \"en\":\n",
    "                annotations.extend(make_doccano_data(speech_id, paragraph_texts, paragraph_ids, map_contents, misses))\n",
    "                if len(paragraph_texts) != len(paragraph_values):\n",
    "                    print(colored(f\"warning: mismatch meta data ({len(paragraph_values)}) vs file ({len(paragraph_texts)}) for file {file_name}\", \"red\"))\n",
    "                nbr_of_files += 1\n",
    "            else:\n",
    "                print(f\"skipping file in language {language}: {file_name}\")\n",
    "                nbr_of_skipped += 1\n",
    "    print(f\"read {nbr_of_files} files; skipped {nbr_of_skipped} file\", end=\"\")\n",
    "    if nbr_of_skipped != 1:\n",
    "        print(\"s\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "    print(misses)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eed6d3ba-c138-429a-9f70-01d9b61159b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_annotations(annotations, file_name):\n",
    "    out_file = open(file_name, \"w\")\n",
    "    for annotation in annotations:\n",
    "        print(json.dumps(annotation), file=out_file)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "798321cf-7c5b-45d3-a03f-4edcd1eb24cc",
   "metadata": {},
   "source": [
    "speech_contents = read_data_file(\"csv/Speech_Contents-20210520.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b56f3cd5-93ac-44e0-9bd7-87d2b13781e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = read_data_file(\"csv/Speeches-20210520.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3293cf9-e082-4226-9295-0b0d9225ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_contents = read_data_file(\"csv/Map_Contents-20200726.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8ff840f7-0f07-4e92-aa63-e29ad36ff7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mwarning: unknown paragraph id nan for document 208; file name: 2012-02-24 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language de: 2015-01-19 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language de: 2013-11-21 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language fr: 2013-04-17 Hollande SFM2020 ann fr.txt\n",
      "skipping file in language fr: 2009-12-01 Sarkozy Elysee (Economy) ann fr.txt\n",
      "\u001b[31mwarning: unknown paragraph id nan for document 335; file name: 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2012-10-17 Thorning Schmidt Statsministeriet ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (21) vs file (20) for file 2014-07-18 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language nl: 2011-10-28 Knot dnb_01 ANN NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (11) vs file (10) for file 2012-07-17 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "skipping file placeholder.txt\n",
      "\u001b[31mwarning: mismatch meta data (2) vs file (1) for file 2010-03-29 Brown sfm2020 ann.txt\u001b[0m\n",
      "skipping file in language fr: 2009-12-14 Sarkozy Elysee (Economy) ann fr.txt\n",
      "skipping file in language fr: 2010-04-20 Barroso European Commission ann fr.txt\n",
      "skipping file in language nl: 2011-09-27 Rutte Rijksoverheid ann.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (13) for file 2013-05-31 Linde Banco de Espana_01 ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (22) vs file (19) for file 2013-10-08 Weidmann Deutsche Bundesbank ann.txt\u001b[0m\n",
      "skipping file in language fr: 2013-02-19 Hollande SFM2020 ann fr.txt\n",
      "\u001b[31mwarning: mismatch meta data (14) vs file (12) for file 2011-05-20 Weidmann Deutsche Bundesbank_01 ANN.txt\u001b[0m\n",
      "skipping file in language fr: 2012-08-30 Hollande SFM2020 ann fr.txt\n",
      "skipping file in language de: 2014-02-27 Merkel Bundesregerung ann g.txt\n",
      "skipping file in language fr: 2011-01-13 Sarkozy gb ann.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-10 Orban London ann.txt\u001b[0m\n",
      "skipping file in language nl: 2011-04-06 Rutte FD evenement ann NL.txt\n",
      "\u001b[31mwarning: mismatch meta data (19) vs file (18) for file 2013-02-26 Rohde Danmarks Nationalbank ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (10) vs file (7) for file 1998-12-14 Schroeder ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (7) vs file (6) for file 2012-12-07 Simor Bis.org ann.txt\u001b[0m\n",
      "skipping file in language de: 2012-01-06 Rutte CSU klausurtagung ann G.txt\n",
      "\u001b[31mwarning: mismatch meta data (4) vs file (3) for file 2011-11-12 Rutte Trilateral Commission - code ann.txt\u001b[0m\n",
      "\u001b[31mwarning: mismatch meta data (24) vs file (22) for file 2012-07-11 Rajoy La Moncloa ann.txt\u001b[0m\n",
      "skipping file in language unk: 2012-07-26 Barroso European Commission.txt\n",
      "read 118 files; skipped 16 files\n",
      "{'Content_Concept_1': 917, 'Content_Concept_2': 346, 'Content_Relation_Explanation': 606, 'Content_Relation_Explanation NaN': 700, 'all': 631, 'both': 417}\n"
     ]
    }
   ],
   "source": [
    "annotations = read_data(speeches, speech_contents, map_contents, text_directory=\"./txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "93dce2fa-5997-409e-af56-742f62bcca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_annotations(annotations, \"annotations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ec802-40ef-418e-8ac8-cffd74ddf40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
